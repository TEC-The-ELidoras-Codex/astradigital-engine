[
  {
    "title": "The Download: the desert data center boom, and how to measure Earth\u2019s elevations",
    "url": "https://www.technologyreview.com/2025/05/22/1117300/the-download-the-desert-data-center-boom-and-how-to-measure-earths-elevations/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Thu, 22 May 2025 12:10:00 +0000",
    "published_datetime": "2025-05-22T13:10:00",
    "summary": "This is today\u2019s edition of\u00a0The Download,\u00a0our weekday newsletter that provides a daily dose of what\u2019s going on in the world of technology.\nThe data center boom in the desert\nIn the high desert east of Reno, Nevada, construction crews are flattening the golden foothills of the Virginia Range, laying the foundations of a data center city.Google, Tract, Switch, EdgeCore, Novva, Vantage, and PowerHouse are all operating, building, or expanding huge facilities nearby. Meanwhile, Microsoft has acquired more than 225 acres of undeveloped property, and Apple is expanding its existing data center just across the Truckee River from the industrial park.\nThe corporate race to amass computing resources to train and run artificial intelligence models and store information in the cloud has sparked a data center boom in the desert\u2014and it\u2019s just far enough away from Nevada\u2019s communities to elude wide notice and, some fear, adequate scrutiny. Read the full story.\n\u2014James Temple\nThis story is part of Power Hungry: AI and our energy future\u2014our new series shining a light on the energy demands and carbon costs of the artificial intelligence revolution. Check out the rest of the package here.\n\nA new atomic clock in space could help us measure elevations on Earth\nIn 2003, engineers from Germany and Switzerland began building a bridge across the Rhine River simultaneously from both sides. Months into construction, they found that the two sides did not meet. The German side hovered 54 centimeters above the Swiss one.\nThe misalignment happened because they measured elevation from sea level differently. To prevent such costly construction errors, in 2015 scientists in the International Association of Geodesy voted to adopt the International Height Reference Frame, or IHRF, a worldwide standard for elevation.Now, a decade after its adoption, scientists are looking to update the standard\u2014by using the most precise clock ever to fly in space. Read the full story.\n\u2014Sophia Chen\n\nThree takeaways about AI\u2019s energy use and climate impacts\n\u2014Casey CrownhartThis week, we published Power Hungry, a package all about AI and energy. At the center of this package is the most comprehensive look yet at AI\u2019s growing power demand, if I do say so myself.\nThis data-heavy story is the result of over six months of reporting by me and my colleague James O\u2019Donnell (and the work of many others on our team). Over that time, with the help of leading researchers, we quantified the energy and emissions impacts of individual queries to AI models and tallied what it all adds up to, both right now and for the years ahead.\nThere\u2019s a lot of data to dig through, and I hope you\u2019ll take the time to explore the whole story. But in the meantime, here are three of my biggest takeaways from working on this project. Read the full story.This article is from The Spark, MIT Technology Review\u2019s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.\n\nMIT Technology Review Narrated: Congress used to evaluate emerging technologies. Let\u2019s do it again.\nArtificial intelligence comes with a shimmer and a sheen of magical thinking. And if we\u2019re not careful, politicians, employers, and other decision-makers may accept at face value the idea that machines can and should replace human judgment and discretion.\nOne way to combat that might be resurrecting the Office of Technology Assessment, a Congressional think tank that detected lies and tested tech until it was shuttered in 1995.\nThis is our latest story to be turned into a MIT Technology Review Narrated podcast, which we\u2019re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it\u2019s released.\n\nThe must-reads\nI\u2019ve combed the internet to find you today\u2019s most fun/important/scary/fascinating stories about technology.\n1 OpenAI is buying Jony Ive\u2019s AI startupThe former Apple design guru will work with Sam Altman to design an entirely new range of devices. (NYT $)+ The deal is worth a whopping $6.5 billion. (Bloomberg $)+ Altman gave OpenAI staff a preview of its AI \u2018companion\u2019 devices. (WSJ $)+ AI products to date have failed to set the world alight. (The Atlantic $)\n2 Microsoft has blocked employee emails containing \u2018Gaza\u2019 or \u2018Palestine\u2019Although the term \u2018Israel\u2019 does not trigger such a block. (The Verge)+ Protest group No Azure for Apartheid has accused the company of censorship. (Fortune $)\n3 DOGE needs to do its work in secretThat\u2019s what the Trump administration is claiming to the Supreme Court, at least. (Ars Technica)+ It\u2019s trying to avoid being forced to hand over internal documents. (NYT $)+ DOGE\u2019s tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)\n4 US banks are racing to embrace cryptocurrencyAhead of new stablecoin legislation. (The Information $)+ Attendees at Trump\u2019s crypto dinner paid over $1 million for the privilege. (NBC News)+ Bitcoin has surged to an all-time peak yet again. (Reuters)\n5 China is making huge technological leapsThanks to the billions it\u2019s poured into narrowing the gap between it and the US. (WSJ $)+ Nvidia\u2019s CEO has branded America\u2019s chip curbs on China \u2018a failure.\u2019 (FT $)+ There can be no winners in a US-China AI arms race. (MIT Technology Review)\n6 Disordered eating content is rife on TikTokBut a pocket of creators are dedicated to debunking the worst of it. (Wired $)\n7 The US military is interested in the world\u2019s largest aircraftThe gigantic WindRunner plane will have an 80-metre wingspan. (New Scientist $)+ Phase two of military AI has arrived. (MIT Technology Review)\n8 How AI is shaking up animationNew tools are slashing the costs of creating episodes by up to 90%. (NYT $)+ Generative AI is reshaping South Korea\u2019s webcomics industry. (MIT Technology Review)\n9 Tesla\u2019s Cybertruck is a flopSorry, Elon. (Fast Company $)+ The vehicles\u2019 resale value is plummeting. (The Daily Beast)\n10 Google\u2019s new AI video generator loves this terrible jokeWhich appears to originate from a Reddit post. (404 Media)+ What happened when 20 comedians got AI to write their routines. (MIT Technology Review)\n\nQuote of the day\n\u201cIt feels like we are marching off a cliff.\u201d\n\u2014An unnamed software engineering vice president jokes that future developers conferences will be attended by the AI agents companies like Microsoft are racing to deploy, Semafor reports.\n\nOne more thing\n\nWhat does GPT-3 \u201cknow\u201d about me?One of the biggest stories in tech is the rise of large language models that produce text that reads like a human might have written it.These models\u2019 power comes from being trained on troves of publicly available human-created text hoovered up from the internet. If you\u2019ve posted anything even remotely personal in English on the internet, chances are your data might be part of some of the world\u2019s most popular LLMs.Melissa Heikkil\u00e4, MIT Technology Review\u2019s former AI reporter, wondered what data these models might have on her\u2014and how it could be misused. So she put OpenAI\u2019s GPT-3 to the test. Read about what she found.\n\nWe can still have nice things\nA place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet \u2019em at me.)\n+ Don\u2019t shoot the messenger, but it seems like there\u2019s a new pizza king in town  ($)+ Ranked: every Final Destination film, from worst to best.+ Who knew that jelly could help to preserve coral reefs? Not I.+ A new generation of space archaeologists are beavering away to document our journeys to the stars.",
    "content": "This is today\u2019s edition of\u00a0The Download,\u00a0our weekday newsletter that provides a daily dose of what\u2019s going on in the world of technology.\nThe data center boom in the desert\nIn the high desert east of Reno, Nevada, construction crews are flattening the golden foothills of the Virginia Range, laying the foundations of a data center city.Google, Tract, Switch, EdgeCore, Novva, Vantage, and PowerHouse are all operating, building, or expanding huge facilities nearby. Meanwhile, Microsoft has acquired more than 225 acres of undeveloped property, and Apple is expanding its existing data center just across the Truckee River from the industrial park.\nThe corporate race to amass computing resources to train and run artificial intelligence models and store information in the cloud has sparked a data center boom in the desert\u2014and it\u2019s just far enough away from Nevada\u2019s communities to elude wide notice and, some fear, adequate scrutiny. Read the full story.\n\u2014James Temple\nThis story is part of Power Hungry: AI and our energy future\u2014our new series shining a light on the energy demands and carbon costs of the artificial intelligence revolution. Check out the rest of the package here.\n\nA new atomic clock in space could help us measure elevations on Earth\nIn 2003, engineers from Germany and Switzerland began building a bridge across the Rhine River simultaneously from both sides. Months into construction, they found that the two sides did not meet. The German side hovered 54 centimeters above the Swiss one.\nThe misalignment happened because they measured elevation from sea level differently. To prevent such costly construction errors, in 2015 scientists in the International Association of Geodesy voted to adopt the International Height Reference Frame, or IHRF, a worldwide standard for elevation.Now, a decade after its adoption, scientists are looking to update the standard\u2014by using the most precise clock ever to fly in space. Read the full story.\n\u2014Sophia Chen\n\nThree takeaways about AI\u2019s energy use and climate impacts\n\u2014Casey CrownhartThis week, we published Power Hungry, a package all about AI and energy. At the center of this package is the most comprehensive look yet at AI\u2019s growing power demand, if I do say so myself.\nThis data-heavy story is the result of over six months of reporting by me and my colleague James O\u2019Donnell (and the work of many others on our team). Over that time, with the help of leading researchers, we quantified the energy and emissions impacts of individual queries to AI models and tallied what it all adds up to, both right now and for the years ahead.\nThere\u2019s a lot of data to dig through, and I hope you\u2019ll take the time to explore the whole story. But in the meantime, here are three of my biggest takeaways from working on this project. Read the full story.This article is from The Spark, MIT Technology Review\u2019s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.\n\nMIT Technology Review Narrated: Congress used to evaluate emerging technologies. Let\u2019s do it again.\nArtificial intelligence comes with a shimmer and a sheen of magical thinking. And if we\u2019re not careful, politicians, employers, and other decision-makers may accept at face value the idea that machines can and should replace human judgment and discretion.\nOne way to combat that might be resurrecting the Office of Technology Assessment, a Congressional think tank that detected lies and tested tech until it was shuttered in 1995.\nThis is our latest story to be turned into a MIT Technology Review Narrated podcast, which we\u2019re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it\u2019s released.\n\nThe must-reads\nI\u2019ve combed the internet to find you today\u2019s most fun/important/scary/fascinating stories about technology.\n1 OpenAI is buying Jony Ive\u2019s AI startupThe former Apple design guru will work with Sam Altman to design an entirely new range of devices. (NYT $)+ The deal is worth a whopping $6.5 billion. (Bloomberg $)+ Altman gave OpenAI staff a preview of its AI \u2018companion\u2019 devices. (WSJ $)+ AI products to date have failed to set the world alight. (The Atlantic $)\n2 Microsoft has blocked employee emails containing \u2018Gaza\u2019 or \u2018Palestine\u2019Although the term \u2018Israel\u2019 does not trigger such a block. (The Verge)+ Protest group No Azure for Apartheid has accused the company of censorship. (Fortune $)\n3 DOGE needs to do its work in secretThat\u2019s what the Trump administration is claiming to the Supreme Court, at least. (Ars Technica)+ It\u2019s trying to avoid being forced to hand over internal documents. (NYT $)+ DOGE\u2019s tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)\n4 US banks are racing to embrace cryptocurrencyAhead of new stablecoin legislation. (The Information $)+ Attendees at Trump\u2019s crypto dinner paid over $1 million for the privilege. (NBC News)+ Bitcoin has surged to an all-time peak yet again. (Reuters)\n5 China is making huge technological leapsThanks to the billions it\u2019s poured into narrowing the gap between it and the US. (WSJ $)+ Nvidia\u2019s CEO has branded America\u2019s chip curbs on China \u2018a failure.\u2019 (FT $)+ There can be no winners in a US-China AI arms race. (MIT Technology Review)\n6 Disordered eating content is rife on TikTokBut a pocket of creators are dedicated to debunking the worst of it. (Wired $)\n7 The US military is interested in the world\u2019s largest aircraftThe gigantic WindRunner plane will have an 80-metre wingspan. (New Scientist $)+ Phase two of military AI has arrived. (MIT Technology Review)\n8 How AI is shaking up animationNew tools are slashing the costs of creating episodes by up to 90%. (NYT $)+ Generative AI is reshaping South Korea\u2019s webcomics industry. (MIT Technology Review)\n9 Tesla\u2019s Cybertruck is a flopSorry, Elon. (Fast Company $)+ The vehicles\u2019 resale value is plummeting. (The Daily Beast)\n10 Google\u2019s new AI video generator loves this terrible jokeWhich appears to originate from a Reddit post. (404 Media)+ What happened when 20 comedians got AI to write their routines. (MIT Technology Review)\n\nQuote of the day\n\u201cIt feels like we are marching off a cliff.\u201d\n\u2014An unnamed software engineering vice president jokes that future developers conferences will be attended by the AI agents companies like Microsoft are racing to deploy, Semafor reports.\n\nOne more thing\n\nWhat does GPT-3 \u201cknow\u201d about me?One of the biggest stories in tech is the rise of large language models that produce text that reads like a human might have written it.These models\u2019 power comes from being trained on troves of publicly available human-created text hoovered up from the internet. If you\u2019ve posted anything even remotely personal in English on the internet, chances are your data might be part of some of the world\u2019s most popular LLMs.Melissa Heikkil\u00e4, MIT Technology Review\u2019s former AI reporter, wondered what data these models might have on her\u2014and how it could be misused. So she put OpenAI\u2019s GPT-3 to the test. Read about what she found.\n\nWe can still have nice things\nA place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet \u2019em at me.)\n+ Don\u2019t shoot the messenger, but it seems like there\u2019s a new pizza king in town  ($)+ Ranked: every Final Destination film, from worst to best.+ Who knew that jelly could help to preserve coral reefs? Not I.+ A new generation of space archaeologists are beavering away to document our journeys to the stars."
  },
  {
    "title": "Three takeaways about AI\u2019s energy use and climate impacts",
    "url": "https://www.technologyreview.com/2025/05/22/1117277/ai-energy-three-takeaways/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Thu, 22 May 2025 09:00:00 +0000",
    "published_datetime": "2025-05-22T10:00:00",
    "summary": "This week, we published Power Hungry, a package all about AI and energy. At the center of this package is the most comprehensive look yet at AI\u2019s growing power demand, if I do say so myself.\u00a0\nThis data-heavy story is the result of over six months of reporting by me and my colleague James O\u2019Donnell (and the work of many others on our team). Over that time, with the help of leading researchers, we quantified the energy and emissions impacts of individual queries to AI models and tallied what it all adds up to, both right now and for the years ahead.\u00a0\nThere\u2019s a lot of data to dig through, and I hope you\u2019ll take the time to explore the whole story. But in the meantime, here are three of my biggest takeaways from working on this project.\u00a0\n1. The energy demands of AI are anything but constant.\u00a0\nIf you\u2019ve heard estimates of AI\u2019s toll, it\u2019s probably a single number associated with a query, likely to OpenAI\u2019s ChatGPT. One popular estimate is that writing an email with ChatGPT uses 500 milliliters (or roughly a bottle) of water. But as we started reporting, I was surprised to learn just how much the details of a query can affect its energy demand. No two queries are the same\u2014for several reasons, including their complexity and the particulars of the model being queried.\nOne key caveat here is that we don\u2019t know much about \u201cclosed source\u201d models\u2014for these, companies hold back the details of how they work. (OpenAI\u2019s ChatGPT and Google\u2019s Gemini are examples.) Instead, we worked with researchers who measured the energy it takes to run open-source AI models, for which the source code is publicly available.\u00a0\nBut using open-source models, it\u2019s possible to directly measure the energy used to respond to a query rather than just guess. We worked with researchers who generated text, images, and video and measured the energy required for the chips the models are based on to perform the task.\u00a0\u00a0\nEven just within the text responses, there was a pretty large range of energy needs. A complicated travel itinerary consumed nearly 10 times as much energy as a simple request for a few jokes, for example. An even bigger difference comes from the size of the model used. Larger models with more parameters used up to 70 times more energy than smaller ones for the same prompts.\u00a0\nAs you might imagine, there\u2019s also a big difference between text, images, or video. Videos generally took hundreds of times more energy to generate than text responses.\u00a0\n2. What\u2019s powering the grid will greatly affect the climate toll of AI\u2019s energy use.\u00a0\nAs the resident climate reporter on this project, I was excited to take the expected energy toll and translate it into an expected emissions burden.\u00a0\nPowering a data center with a nuclear reactor or a whole bunch of solar panels and batteries will not affect our planet the same way as burning mountains of coal. To quantify this idea, we used a figure called carbon intensity, a measure of how dirty a unit of electricity is on a given grid.\u00a0\nWe found that the same exact query, with the same exact energy demand, will have a very different climate impact depending on what the data center is powered by, and that depends on the location and the time of day. For example, querying a data center in West Virginia could cause nearly twice the emissions of querying one in California, according to calculations based on average data from 2024.\nThis point shows why it matters where tech giants are building data centers, what the grid looks like in their chosen locations, and how that might change with more demand from the new infrastructure.\u00a0\n3. There is still so much that we don\u2019t know when it comes to AI and energy.\u00a0\nOur reporting resulted in estimates that are some of the most specific and comprehensive out there. But ultimately, we still have no idea what many of the biggest, most influential models are adding up to in terms of energy and emissions. None of the companies we reached out to were willing to provide numbers during our reporting. Not one.\nAdding up our estimates can only go so far, in part because AI is increasingly everywhere. While today you might generally have to go to a dedicated site and type in questions, in the future AI could be stitched into the fabric of our interactions with technology. (See my colleague Will Douglas Heaven\u2019s new story on Google\u2019s I/O showcase: \u201cBy putting AI into everything, Google wants to make it invisible.\u201d)\nAI could be one of the major forces that shape our society, our work, and our power grid. Knowing more about its consequences could be crucial to planning our future.\u00a0\nTo dig into our reporting, give the main story a read. And if you\u2019re looking for more details on how we came up with our numbers, you can check out this behind-the-scenes piece.\nThere are also some great related stories in this package, including one from James Temple on the data center boom in the Nevada desert, one from David Rotman about how AI\u2019s rise could entrench natural gas, and one from Will Douglas Heaven on a few technical innovations that could help make AI more efficient. Oh, and I also have a piece on why nuclear isn\u2019t the easy answer some think it is.\u00a0\nFind them, and the rest of the stories in the package, here.\u00a0\nThis article is from The Spark, MIT Technology Review\u2019s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.\n",
    "content": "This week, we published Power Hungry, a package all about AI and energy. At the center of this package is the most comprehensive look yet at AI\u2019s growing power demand, if I do say so myself.\u00a0\nThis data-heavy story is the result of over six months of reporting by me and my colleague James O\u2019Donnell (and the work of many others on our team). Over that time, with the help of leading researchers, we quantified the energy and emissions impacts of individual queries to AI models and tallied what it all adds up to, both right now and for the years ahead.\u00a0\nThere\u2019s a lot of data to dig through, and I hope you\u2019ll take the time to explore the whole story. But in the meantime, here are three of my biggest takeaways from working on this project.\u00a0\n1. The energy demands of AI are anything but constant.\u00a0\nIf you\u2019ve heard estimates of AI\u2019s toll, it\u2019s probably a single number associated with a query, likely to OpenAI\u2019s ChatGPT. One popular estimate is that writing an email with ChatGPT uses 500 milliliters (or roughly a bottle) of water. But as we started reporting, I was surprised to learn just how much the details of a query can affect its energy demand. No two queries are the same\u2014for several reasons, including their complexity and the particulars of the model being queried.\nOne key caveat here is that we don\u2019t know much about \u201cclosed source\u201d models\u2014for these, companies hold back the details of how they work. (OpenAI\u2019s ChatGPT and Google\u2019s Gemini are examples.) Instead, we worked with researchers who measured the energy it takes to run open-source AI models, for which the source code is publicly available.\u00a0\nBut using open-source models, it\u2019s possible to directly measure the energy used to respond to a query rather than just guess. We worked with researchers who generated text, images, and video and measured the energy required for the chips the models are based on to perform the task.\u00a0\u00a0\nEven just within the text responses, there was a pretty large range of energy needs. A complicated travel itinerary consumed nearly 10 times as much energy as a simple request for a few jokes, for example. An even bigger difference comes from the size of the model used. Larger models with more parameters used up to 70 times more energy than smaller ones for the same prompts.\u00a0\nAs you might imagine, there\u2019s also a big difference between text, images, or video. Videos generally took hundreds of times more energy to generate than text responses.\u00a0\n2. What\u2019s powering the grid will greatly affect the climate toll of AI\u2019s energy use.\u00a0\nAs the resident climate reporter on this project, I was excited to take the expected energy toll and translate it into an expected emissions burden.\u00a0\nPowering a data center with a nuclear reactor or a whole bunch of solar panels and batteries will not affect our planet the same way as burning mountains of coal. To quantify this idea, we used a figure called carbon intensity, a measure of how dirty a unit of electricity is on a given grid.\u00a0\nWe found that the same exact query, with the same exact energy demand, will have a very different climate impact depending on what the data center is powered by, and that depends on the location and the time of day. For example, querying a data center in West Virginia could cause nearly twice the emissions of querying one in California, according to calculations based on average data from 2024.\nThis point shows why it matters where tech giants are building data centers, what the grid looks like in their chosen locations, and how that might change with more demand from the new infrastructure.\u00a0\n3. There is still so much that we don\u2019t know when it comes to AI and energy.\u00a0\nOur reporting resulted in estimates that are some of the most specific and comprehensive out there. But ultimately, we still have no idea what many of the biggest, most influential models are adding up to in terms of energy and emissions. None of the companies we reached out to were willing to provide numbers during our reporting. Not one.\nAdding up our estimates can only go so far, in part because AI is increasingly everywhere. While today you might generally have to go to a dedicated site and type in questions, in the future AI could be stitched into the fabric of our interactions with technology. (See my colleague Will Douglas Heaven\u2019s new story on Google\u2019s I/O showcase: \u201cBy putting AI into everything, Google wants to make it invisible.\u201d)\nAI could be one of the major forces that shape our society, our work, and our power grid. Knowing more about its consequences could be crucial to planning our future.\u00a0\nTo dig into our reporting, give the main story a read. And if you\u2019re looking for more details on how we came up with our numbers, you can check out this behind-the-scenes piece.\nThere are also some great related stories in this package, including one from James Temple on the data center boom in the Nevada desert, one from David Rotman about how AI\u2019s rise could entrench natural gas, and one from Will Douglas Heaven on a few technical innovations that could help make AI more efficient. Oh, and I also have a piece on why nuclear isn\u2019t the easy answer some think it is.\u00a0\nFind them, and the rest of the stories in the package, here.\u00a0\nThis article is from The Spark, MIT Technology Review\u2019s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.\n"
  },
  {
    "title": "A new atomic clock in space could help us measure elevations on Earth",
    "url": "https://www.technologyreview.com/2025/05/22/1117294/a-new-atomic-clock-in-space-could-help-us-measure-elevations-on-earth/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Thu, 22 May 2025 09:00:00 +0000",
    "published_datetime": "2025-05-22T10:00:00",
    "summary": "In 2003, engineers from Germany and Switzerland began building a bridge across the Rhine River simultaneously from both sides. Months into construction, they found that the two sides did not meet. The German side hovered 54 centimeters above the Swiss side.\nThe misalignment occurred because the German engineers had measured elevation with a historic level of the North Sea as its zero point, while the Swiss ones had used the Mediterranean Sea, which was 27 centimeters lower. We may speak colloquially of elevations with respect to \u201csea level,\u201d but Earth\u2019s seas are actually not level. \u201cThe sea level is varying from location to location,\u201d says Laura Sanchez, a geodesist at the Technical University of Munich in Germany. (Geodesists study our planet\u2019s shape, orientation, and gravitational field.) While the two teams knew about the 27-centimeter difference, they mixed up which side was higher. Ultimately, Germany lowered its side to complete the bridge.\u00a0\nTo prevent such costly construction errors, in 2015 scientists in the International Association of Geodesy voted to adopt the International Height Reference Frame, or IHRF, a worldwide standard for elevation. It\u2019s the third-dimensional counterpart to latitude and longitude, says Sanchez, who helps coordinate the standardization effort.\u00a0\nNow, a decade after its adoption, geodesists are looking to update the standard\u2014by using the most precise clock ever to fly in space.\nThat clock, called the Atomic Clock Ensemble in Space, or ACES, launched into orbit from Florida last month, bound for the International Space Station. ACES, which was built by the European Space Agency, consists of two connected atomic clocks, one containing cesium atoms and the other containing hydrogen, combined to produce a single set of ticks with higher precision than either clock alone.\u00a0\nPendulum clocks are only accurate to about a second per day, as the rate at which a pendulum swings can vary with humidity, temperature, and the weight of extra dust. Atomic clocks in current GPS satellites will lose or gain a second on average every 3,000 years. ACES, on the other hand, \u201cwill not lose or gain a second in 300 million years,\u201d says Luigi Cacciapuoti, an ESA physicist who helped build and launch the device. (In 2022, China installed a potentially stabler clock on its space station, but the Chinese government has not publicly shared the clock\u2019s performance after launch, according to Cacciapuoti.)\u00a0\nFrom space, ACES will link to some of the most accurate clocks on Earth to create a synchronized clock network, which will support its main purpose: to perform tests of fundamental physics.\u00a0\nBut it\u2019s of special interest for geodesists because it can be used to make gravitational measurements that will help establish a more precise zero point from which to measure elevation across the world.\nAlignment over this \u201czero point\u201d (basically where you stick the end of the tape measure to measure elevation) is important for international collaboration. It makes it easier, for example, to monitor and compare sea-level changes around the world. It is especially useful for building infrastructure involving flowing water, such as dams and canals. In 2020, the international height standard even resolved a long-standing dispute between China and Nepal over Mount Everest\u2019s height. For years, China said the mountain was 8,844.43 meters; Nepal measured it at 8,848. Using the IHRF, the two countries finally agreed that the mountain was 8,848.86\u2009meters.\u00a0\n\nA worker performs tests on ACES at a cleanroom at the Kennedy Space Center in Florida.ESA-T. PEIGNIER\n\n\nTo create a standard zero point, geodesists create a model of Earth known as a geoid. Every point on the surface of this lumpy, potato-shaped model experiences the same gravity, which means that if you dug a canal at the height of the geoid, the water within the canal would be level and would not flow. Distance from the geoid establishes a global system for altitude.\nHowever, the current model lacks precision, particularly in Africa and South America, says Sanchez. Today\u2019s geoid has been built using instruments that directly measure Earth\u2019s gravity. These have been carried on satellites, which excel at getting a global but low-resolution view, and have also been used to get finer details via expensive ground- and airplane-based surveys. But geodesists have not had the funding to survey Africa and South America as extensively as other parts of the world, particularly in difficult terrain such as the Amazon rainforest and Sahara Desert.\u00a0\nTo understand the discrepancy in precision, imagine a bridge that spans Africa from the Mediterranean coast to Cape Town, South Africa. If it\u2019s built using the current geoid, the two ends of the bridge will be misaligned by tens of centimeters. In comparison, you\u2019d be off by at most five centimeters if you were building a bridge spanning North America.\u00a0\nTo improve the geoid\u2019s precision, geodesists want to create a worldwide network of clocks, synchronized from space. The idea works according to Einstein\u2019s theory of general relativity, which states that the stronger the gravitational field, the more slowly time passes. The 2014 sci-fi movie Interstellar illustrates an extreme version of this so-called time dilation: Two astronauts spend a few hours in extreme gravity near a black hole to return to a shipmate who has aged more than two decades. Similarly, Earth\u2019s gravity grows weaker the higher in elevation you are. Your feet, for example, experience slightly stronger gravity than your head when you\u2019re standing. Assuming you live to be about 80 years old, over a lifetime your head will age tens of billionths of a second more than your feet.\u00a0\nA clock network would allow geodesists to compare the ticking of clocks all over the world. They could then use the variations in time to map Earth\u2019s gravitational field much more precisely, and consequently create a more precise geoid. The most accurate clocks today are precise enough to measure variations in time that map onto centimeter-level differences in elevation.\u00a0\n\u201cWe want to have the accuracy level at the one-centimeter or sub-centimeter level,\u201d says J\u00fcrgen M\u00fcller, a geodesist at Leibniz University Hannover in Germany. Specifically, geodesists would use the clock measurements to validate their geoid model, which they currently do with ground- and plane-based surveying techniques. They think that a clock network should be considerably less expensive.\nACES is just a first step. It is capable of measuring altitudes at various points around Earth with 10-centimeter precision, says Cacciapuoti. But the point of ACES is to prototype the clock network. It will demonstrate the optical and microwave technology needed to use a clock in space to connect some of the most advanced ground-based clocks together. In the next year or so, M\u00fcller plans to use ACES to connect to clocks on the ground, starting with three in Germany. M\u00fcller\u2019s team could then make more precise measurements at the location of those clocks.\nThese early studies will pave the way for work connecting even more precise clocks than ACES to the network, ultimately leading to an improved geoid. The best clocks today are some 50 times more precise than ACES. \u201cThe exciting thing is that clocks are getting even stabler,\u201d says Michael Bevis, a geodesist at Ohio State University, who was not involved with the project. A more precise geoid would allow engineers, for example, to build a canal with better control of its depth and flow, he says. However, he points out that in order for geodesists to take advantage of the clocks\u2019 precision, they will also have to improve their mathematical models of Earth\u2019s gravitational field.\u00a0\nEven starting to build this clock network has required decades of dedicated work by scientists and engineers. It took ESA three decades to make a clock as small as ACES that is suitable for space, says Cacciapuoti. This meant miniaturizing a clock the size of a laboratory into the size of a small fridge. \u201cIt was a huge engineering effort,\u201d says Cacciapuoti, who has been working on the project since he began at ESA 20 years ago.\u00a0\nGeodesists expect they\u2019ll need at least another decade to develop the clock network and launch more clocks into space. One possibility would be to slot the clocks onto GPS satellites. The timeline depends on the success of the ACES mission and the willingness of government agencies to invest, says Sanchez. But whatever the specifics, mapping the world takes time.",
    "content": "In 2003, engineers from Germany and Switzerland began building a bridge across the Rhine River simultaneously from both sides. Months into construction, they found that the two sides did not meet. The German side hovered 54 centimeters above the Swiss side.\nThe misalignment occurred because the German engineers had measured elevation with a historic level of the North Sea as its zero point, while the Swiss ones had used the Mediterranean Sea, which was 27 centimeters lower. We may speak colloquially of elevations with respect to \u201csea level,\u201d but Earth\u2019s seas are actually not level. \u201cThe sea level is varying from location to location,\u201d says Laura Sanchez, a geodesist at the Technical University of Munich in Germany. (Geodesists study our planet\u2019s shape, orientation, and gravitational field.) While the two teams knew about the 27-centimeter difference, they mixed up which side was higher. Ultimately, Germany lowered its side to complete the bridge.\u00a0\nTo prevent such costly construction errors, in 2015 scientists in the International Association of Geodesy voted to adopt the International Height Reference Frame, or IHRF, a worldwide standard for elevation. It\u2019s the third-dimensional counterpart to latitude and longitude, says Sanchez, who helps coordinate the standardization effort.\u00a0\nNow, a decade after its adoption, geodesists are looking to update the standard\u2014by using the most precise clock ever to fly in space.\nThat clock, called the Atomic Clock Ensemble in Space, or ACES, launched into orbit from Florida last month, bound for the International Space Station. ACES, which was built by the European Space Agency, consists of two connected atomic clocks, one containing cesium atoms and the other containing hydrogen, combined to produce a single set of ticks with higher precision than either clock alone.\u00a0\nPendulum clocks are only accurate to about a second per day, as the rate at which a pendulum swings can vary with humidity, temperature, and the weight of extra dust. Atomic clocks in current GPS satellites will lose or gain a second on average every 3,000 years. ACES, on the other hand, \u201cwill not lose or gain a second in 300 million years,\u201d says Luigi Cacciapuoti, an ESA physicist who helped build and launch the device. (In 2022, China installed a potentially stabler clock on its space station, but the Chinese government has not publicly shared the clock\u2019s performance after launch, according to Cacciapuoti.)\u00a0\nFrom space, ACES will link to some of the most accurate clocks on Earth to create a synchronized clock network, which will support its main purpose: to perform tests of fundamental physics.\u00a0\nBut it\u2019s of special interest for geodesists because it can be used to make gravitational measurements that will help establish a more precise zero point from which to measure elevation across the world.\nAlignment over this \u201czero point\u201d (basically where you stick the end of the tape measure to measure elevation) is important for international collaboration. It makes it easier, for example, to monitor and compare sea-level changes around the world. It is especially useful for building infrastructure involving flowing water, such as dams and canals. In 2020, the international height standard even resolved a long-standing dispute between China and Nepal over Mount Everest\u2019s height. For years, China said the mountain was 8,844.43 meters; Nepal measured it at 8,848. Using the IHRF, the two countries finally agreed that the mountain was 8,848.86\u2009meters.\u00a0\n\nA worker performs tests on ACES at a cleanroom at the Kennedy Space Center in Florida.ESA-T. PEIGNIER\n\n\nTo create a standard zero point, geodesists create a model of Earth known as a geoid. Every point on the surface of this lumpy, potato-shaped model experiences the same gravity, which means that if you dug a canal at the height of the geoid, the water within the canal would be level and would not flow. Distance from the geoid establishes a global system for altitude.\nHowever, the current model lacks precision, particularly in Africa and South America, says Sanchez. Today\u2019s geoid has been built using instruments that directly measure Earth\u2019s gravity. These have been carried on satellites, which excel at getting a global but low-resolution view, and have also been used to get finer details via expensive ground- and airplane-based surveys. But geodesists have not had the funding to survey Africa and South America as extensively as other parts of the world, particularly in difficult terrain such as the Amazon rainforest and Sahara Desert.\u00a0\nTo understand the discrepancy in precision, imagine a bridge that spans Africa from the Mediterranean coast to Cape Town, South Africa. If it\u2019s built using the current geoid, the two ends of the bridge will be misaligned by tens of centimeters. In comparison, you\u2019d be off by at most five centimeters if you were building a bridge spanning North America.\u00a0\nTo improve the geoid\u2019s precision, geodesists want to create a worldwide network of clocks, synchronized from space. The idea works according to Einstein\u2019s theory of general relativity, which states that the stronger the gravitational field, the more slowly time passes. The 2014 sci-fi movie Interstellar illustrates an extreme version of this so-called time dilation: Two astronauts spend a few hours in extreme gravity near a black hole to return to a shipmate who has aged more than two decades. Similarly, Earth\u2019s gravity grows weaker the higher in elevation you are. Your feet, for example, experience slightly stronger gravity than your head when you\u2019re standing. Assuming you live to be about 80 years old, over a lifetime your head will age tens of billionths of a second more than your feet.\u00a0\nA clock network would allow geodesists to compare the ticking of clocks all over the world. They could then use the variations in time to map Earth\u2019s gravitational field much more precisely, and consequently create a more precise geoid. The most accurate clocks today are precise enough to measure variations in time that map onto centimeter-level differences in elevation.\u00a0\n\u201cWe want to have the accuracy level at the one-centimeter or sub-centimeter level,\u201d says J\u00fcrgen M\u00fcller, a geodesist at Leibniz University Hannover in Germany. Specifically, geodesists would use the clock measurements to validate their geoid model, which they currently do with ground- and plane-based surveying techniques. They think that a clock network should be considerably less expensive.\nACES is just a first step. It is capable of measuring altitudes at various points around Earth with 10-centimeter precision, says Cacciapuoti. But the point of ACES is to prototype the clock network. It will demonstrate the optical and microwave technology needed to use a clock in space to connect some of the most advanced ground-based clocks together. In the next year or so, M\u00fcller plans to use ACES to connect to clocks on the ground, starting with three in Germany. M\u00fcller\u2019s team could then make more precise measurements at the location of those clocks.\nThese early studies will pave the way for work connecting even more precise clocks than ACES to the network, ultimately leading to an improved geoid. The best clocks today are some 50 times more precise than ACES. \u201cThe exciting thing is that clocks are getting even stabler,\u201d says Michael Bevis, a geodesist at Ohio State University, who was not involved with the project. A more precise geoid would allow engineers, for example, to build a canal with better control of its depth and flow, he says. However, he points out that in order for geodesists to take advantage of the clocks\u2019 precision, they will also have to improve their mathematical models of Earth\u2019s gravitational field.\u00a0\nEven starting to build this clock network has required decades of dedicated work by scientists and engineers. It took ESA three decades to make a clock as small as ACES that is suitable for space, says Cacciapuoti. This meant miniaturizing a clock the size of a laboratory into the size of a small fridge. \u201cIt was a huge engineering effort,\u201d says Cacciapuoti, who has been working on the project since he began at ESA 20 years ago.\u00a0\nGeodesists expect they\u2019ll need at least another decade to develop the clock network and launch more clocks into space. One possibility would be to slot the clocks onto GPS satellites. The timeline depends on the success of the ACES mission and the willingness of government agencies to invest, says Sanchez. But whatever the specifics, mapping the world takes time."
  },
  {
    "title": "Roundtables: A New Look at AI\u2019s Energy Use",
    "url": "https://www.technologyreview.com/2025/05/21/1117287/roundtables-a-new-look-at-ais-energy-use-2/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Wed, 21 May 2025 19:45:00 +0000",
    "published_datetime": "2025-05-21T20:45:00",
    "summary": "Recorded on May 21, 2025\nBig Tech\u2019s appetite for energy is growing rapidly as adoption of AI accelerates. But just how much energy does even a single AI query use? And what does it mean for the climate? Hear from\u00a0MIT Technology Review\u00a0editor in chief Mat Honan, senior climate reporter Casey Crownhart, and AI reporter James O\u2019Donnell as they explore AI\u2019s energy demands now and in the future.\nSpeakers:\u00a0Mat Honan, editor in chief, Casey Crownhart, climate reporter, and James O\u2019Donnell, AI reporter.\n\n\nRelated Coverage: \n\nPower Hungry: AI and our energy future\nWe did the math on AI\u2019s energy footprint. Here\u2019s the story you haven\u2019t\u00a0heard.\nEverything you need to know about estimating AI\u2019s energy and emissions burden\nThese four charts sum up the state of AI and energy\n\n",
    "content": "Recorded on May 21, 2025\nBig Tech\u2019s appetite for energy is growing rapidly as adoption of AI accelerates. But just how much energy does even a single AI query use? And what does it mean for the climate? Hear from\u00a0MIT Technology Review\u00a0editor in chief Mat Honan, senior climate reporter Casey Crownhart, and AI reporter James O\u2019Donnell as they explore AI\u2019s energy demands now and in the future.\nSpeakers:\u00a0Mat Honan, editor in chief, Casey Crownhart, climate reporter, and James O\u2019Donnell, AI reporter.\n\n\nRelated Coverage: \n\nPower Hungry: AI and our energy future\nWe did the math on AI\u2019s energy footprint. Here\u2019s the story you haven\u2019t\u00a0heard.\nEverything you need to know about estimating AI\u2019s energy and emissions burden\nThese four charts sum up the state of AI and energy\n\n"
  },
  {
    "title": "The Download: Google\u2019s AI mission, and America\u2019s reliance on natural gas",
    "url": "https://www.technologyreview.com/2025/05/21/1117249/the-download-googles-ai-mission-and-americas-reliance-on-natural-gas/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Wed, 21 May 2025 12:20:00 +0000",
    "published_datetime": "2025-05-21T13:20:00",
    "summary": "This is today\u2019s edition of\u00a0The Download,\u00a0our weekday newsletter that provides a daily dose of what\u2019s going on in the world of technology.\nBy putting AI into everything, Google wants to make it invisible\nIf you want to know where AI is headed, this year\u2019s Google I/O has you covered. The company\u2019s annual showcase of next-gen products, which kicked off yesterday, has all of the pomp and pizzazz, the sizzle reels and celebrity walk-ons, that you\u2019d expect from a multimillion dollar marketing event.But it also shows us just how fast this still-experimental technology is being subsumed into a line-up designed to sell phones and subscription tiers. Never before have I seen this thing we call artificial intelligence appear so normal. Read the full story.\n\u2014Will Douglas Heaven\n\nAI could keep us dependent on natural gas for decades to come\nLast December, Meta announced plans to build a massive $10 billion data center for training its artificial intelligence models in rural northeast Louisiana. Stretching for more than a mile, it will be Meta\u2019s largest in the world, and it will have an enormous appetite for electricity.To power the data center, a Meta contractor called Entergy will build three large natural-gas power plants with a total capacity of 2.3 gigawatts. It\u2019ll also upgrade the grid to accommodate the huge jump in anticipated demand.The choice of natural gas as the go-to solution to meet the growing demand for power from AI is not unique to Louisiana. The fossil fuel is already the country\u2019s chief source of electricity generation, and large natural-gas plants are being built around the country to feed electricity to new and planned AI data centers. That\u2019s all but wiping out any prospect that the US will wean itself off natural gas anytime soon. Read the full story.\n\u2014David Rotman\nThis story is part of Power Hungry: AI and our energy future\u2014our new series shining a light on AI\u2019s energy usage. Check out the rest of the package here.\n\nTake a new look at AI\u2019s energy use\nBig Tech\u2019s appetite for energy is growing rapidly as adoption of AI accelerates. But just how much energy does a single AI query use? And what does it mean for the climate?\u00a0Join editor in chief Mat Honan, senior climate reporter Casey Crownhart, and AI reporter James O\u2019Donnell at 1.30pm ET today for a subscriber-only Roundtables conversation digging into our new package of stories about AI\u2019s energy demands now and in the future. Register here.\n\nThe must-reads\nI\u2019ve combed the internet to find you today\u2019s most fun/important/scary/fascinating stories about technology.\n1 Democrats are on the hunt for a digital thought leaderThey\u2019re (finally) realizing how far they\u2019re lagging behind their opponents\u2019 online efforts these days. (NYT $)\u00a0+ AI\u2019s impact on elections is being overblown. (MIT Technology Review)\n2 At least two newspapers printed an AI-generated summer reading list The only problem is, some of the books don\u2019t actually exist. (404 Media)+ It\u2019s a useful reminder to never take anything chatbots produce as fact. (Axios)+ Even regional newspapers aren\u2019t safe from AI slop. (The Atlantic $)+ Why AI hallucinates, and why we can\u2019t stop it. (MIT Technology Review)3 The Earth may already be too hot to maintain polar ice sheetsEven if it stays at current temperature levels. (WP $)+ Why climate researchers are taking the temperature of mountain snow. (MIT Technology Review)\n4 How New York City\u2019s child abuse algorithm flags families for investigationCritics believe it\u2019s open to racial bias. (The Markup)\n5 Here\u2019s what it\u2019s like to interview for a job at DOGE\u00a0\u00a0The hiring process is remarkably fast, for a government entity. (Wired $)+ The department reportedly tried to enter the US government\u2019s publishing operation. (Politico)+ DOGE\u2019s tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)\n6 Fortnite has finally returned to Apple\u2019s App StoreAfter five years and a lengthy legal battle. (NYT $)+ The recent ruling has major implications for the iOS economy. (Reuters)\n7 Most chatbots can be tricked into dispensing dangerous informationFrom hacking advice, to describing how to make drugs. (The Guardian)+ Anthropic has a new way to protect large language models against jailbreaks. (MIT Technology Review)\n8 Young Indonesians are being trafficked to scam farmsFraudulent job ads on Telegram and Facebook lure them into a life of crime. (Rest of World)+ Inside a romance scam compound\u2014and how people get tricked into being there. (MIT Technology Review)\n9\u00a0 Inside the building in China where stolen western iPhones are stripped and soldYou\u2019ll find a buyer for every single component inside the Feiyang Times. (FT $)\n10 Amazon has started randomly refunding customers for old purchasesSome orders were placed as far back as 2018. (Bloomberg $)\n\nQuote of the day\n\u201cAnybody who\u2019s a computer scientist should not be retired right now. They should be working on AI.\u201d\n\u2014Google cofounder Sergey Brin says people with the right technical skills should copy him and quit being retired, TechCrunch reports.\n\nOne more thing\n\nThis fuel plant will use agricultural waste to combat climate changeA startup called Mote plans to build a new type of fuel-producing plant in California\u2019s fertile Central Valley that would, if it works as hoped, continually capture and bury carbon dioxide.It\u2019s among a growing number of efforts to commercialize a concept first proposed two decades ago as a means of combating climate change, known as bioenergy with carbon capture and sequestration, or BECCS.It\u2019s an ambitious plan. However, there are serious challenges to doing BECCS affordably and in ways that reliably suck down significant levels of carbon dioxide. Read the full story.\n\u2014James Temple\n\nWe can still have nice things\nA place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet \u2019em at me.)\n+ These creepy little Labubu toys are everywhere. But why?+ Happy 25th birthday to one of London\u2019s finest institutions, the Tate Modern gallery.+ Why the Mission Impossible film franchise just won\u2019t die.+ Hummingbirds can fly backwards!? Wow.",
    "content": "This is today\u2019s edition of\u00a0The Download,\u00a0our weekday newsletter that provides a daily dose of what\u2019s going on in the world of technology.\nBy putting AI into everything, Google wants to make it invisible\nIf you want to know where AI is headed, this year\u2019s Google I/O has you covered. The company\u2019s annual showcase of next-gen products, which kicked off yesterday, has all of the pomp and pizzazz, the sizzle reels and celebrity walk-ons, that you\u2019d expect from a multimillion dollar marketing event.But it also shows us just how fast this still-experimental technology is being subsumed into a line-up designed to sell phones and subscription tiers. Never before have I seen this thing we call artificial intelligence appear so normal. Read the full story.\n\u2014Will Douglas Heaven\n\nAI could keep us dependent on natural gas for decades to come\nLast December, Meta announced plans to build a massive $10 billion data center for training its artificial intelligence models in rural northeast Louisiana. Stretching for more than a mile, it will be Meta\u2019s largest in the world, and it will have an enormous appetite for electricity.To power the data center, a Meta contractor called Entergy will build three large natural-gas power plants with a total capacity of 2.3 gigawatts. It\u2019ll also upgrade the grid to accommodate the huge jump in anticipated demand.The choice of natural gas as the go-to solution to meet the growing demand for power from AI is not unique to Louisiana. The fossil fuel is already the country\u2019s chief source of electricity generation, and large natural-gas plants are being built around the country to feed electricity to new and planned AI data centers. That\u2019s all but wiping out any prospect that the US will wean itself off natural gas anytime soon. Read the full story.\n\u2014David Rotman\nThis story is part of Power Hungry: AI and our energy future\u2014our new series shining a light on AI\u2019s energy usage. Check out the rest of the package here.\n\nTake a new look at AI\u2019s energy use\nBig Tech\u2019s appetite for energy is growing rapidly as adoption of AI accelerates. But just how much energy does a single AI query use? And what does it mean for the climate?\u00a0Join editor in chief Mat Honan, senior climate reporter Casey Crownhart, and AI reporter James O\u2019Donnell at 1.30pm ET today for a subscriber-only Roundtables conversation digging into our new package of stories about AI\u2019s energy demands now and in the future. Register here.\n\nThe must-reads\nI\u2019ve combed the internet to find you today\u2019s most fun/important/scary/fascinating stories about technology.\n1 Democrats are on the hunt for a digital thought leaderThey\u2019re (finally) realizing how far they\u2019re lagging behind their opponents\u2019 online efforts these days. (NYT $)\u00a0+ AI\u2019s impact on elections is being overblown. (MIT Technology Review)\n2 At least two newspapers printed an AI-generated summer reading list The only problem is, some of the books don\u2019t actually exist. (404 Media)+ It\u2019s a useful reminder to never take anything chatbots produce as fact. (Axios)+ Even regional newspapers aren\u2019t safe from AI slop. (The Atlantic $)+ Why AI hallucinates, and why we can\u2019t stop it. (MIT Technology Review)3 The Earth may already be too hot to maintain polar ice sheetsEven if it stays at current temperature levels. (WP $)+ Why climate researchers are taking the temperature of mountain snow. (MIT Technology Review)\n4 How New York City\u2019s child abuse algorithm flags families for investigationCritics believe it\u2019s open to racial bias. (The Markup)\n5 Here\u2019s what it\u2019s like to interview for a job at DOGE\u00a0\u00a0The hiring process is remarkably fast, for a government entity. (Wired $)+ The department reportedly tried to enter the US government\u2019s publishing operation. (Politico)+ DOGE\u2019s tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)\n6 Fortnite has finally returned to Apple\u2019s App StoreAfter five years and a lengthy legal battle. (NYT $)+ The recent ruling has major implications for the iOS economy. (Reuters)\n7 Most chatbots can be tricked into dispensing dangerous informationFrom hacking advice, to describing how to make drugs. (The Guardian)+ Anthropic has a new way to protect large language models against jailbreaks. (MIT Technology Review)\n8 Young Indonesians are being trafficked to scam farmsFraudulent job ads on Telegram and Facebook lure them into a life of crime. (Rest of World)+ Inside a romance scam compound\u2014and how people get tricked into being there. (MIT Technology Review)\n9\u00a0 Inside the building in China where stolen western iPhones are stripped and soldYou\u2019ll find a buyer for every single component inside the Feiyang Times. (FT $)\n10 Amazon has started randomly refunding customers for old purchasesSome orders were placed as far back as 2018. (Bloomberg $)\n\nQuote of the day\n\u201cAnybody who\u2019s a computer scientist should not be retired right now. They should be working on AI.\u201d\n\u2014Google cofounder Sergey Brin says people with the right technical skills should copy him and quit being retired, TechCrunch reports.\n\nOne more thing\n\nThis fuel plant will use agricultural waste to combat climate changeA startup called Mote plans to build a new type of fuel-producing plant in California\u2019s fertile Central Valley that would, if it works as hoped, continually capture and bury carbon dioxide.It\u2019s among a growing number of efforts to commercialize a concept first proposed two decades ago as a means of combating climate change, known as bioenergy with carbon capture and sequestration, or BECCS.It\u2019s an ambitious plan. However, there are serious challenges to doing BECCS affordably and in ways that reliably suck down significant levels of carbon dioxide. Read the full story.\n\u2014James Temple\n\nWe can still have nice things\nA place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet \u2019em at me.)\n+ These creepy little Labubu toys are everywhere. But why?+ Happy 25th birthday to one of London\u2019s finest institutions, the Tate Modern gallery.+ Why the Mission Impossible film franchise just won\u2019t die.+ Hummingbirds can fly backwards!? Wow."
  },
  {
    "title": "By putting AI into everything, Google wants to make it invisible",
    "url": "https://www.technologyreview.com/2025/05/21/1117251/by-putting-ai-into-everything-google-wants-to-make-it-invisible/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Wed, 21 May 2025 11:49:03 +0000",
    "published_datetime": "2025-05-21T12:49:03",
    "summary": "If you want to know where AI is headed, this year\u2019s Google I/O has you covered. The company\u2019s annual showcase of next-gen products, which kicked off yesterday, has all of the pomp and pizzazz, the sizzle reels and celebrity walk-ons, that you\u2019d expect from a multimillion-dollar marketing event.\nBut it also shows us just how fast this still experimental technology is being subsumed into a lineup designed to sell phones and subscription tiers. Never before have I seen this thing we call artificial intelligence appear so normal.\nYes, Google\u2019s roster of consumer-facing products is the slickest on offer. The firm is bundling most of its multimodal models into its Gemini app, including the new Imagen 4 image generator and the new Veo 3 video generator. That means you can now access Google\u2019s full range of generative models via a single chatbot. It also announced Gemini Live, a feature that lets you share your phone\u2019s screen or your camera\u2019s view with the chatbot and ask it about what it can see.\nThose features were previously only seen in demos of Project Astra, a \u201cuniversal AI assistant\u201c that Google DeepMind is working on. Now, Google is inching toward putting Project Astra into the hands of anyone with a smartphone.\nGoogle is also rolling out AI Mode, an LLM-powered front end to search. This can now pull in personal information from Gmail or Google Docs to tailor searches to users. It will include Deep Search, which can break a query down into hundreds of individual searches and then summarize the results; a version of Project Mariner, Google DeepMind\u2019s browser-using agent; and Search Live, which lets you hold up your camera and ask it what it sees.\nThis is the new frontier. It\u2019s no longer about who has the most powerful models, but who can spin them into the best products. OpenAI\u2019s ChatGPT includes many similar features to Gemini\u2019s. But with its existing ecosystem of consumer services and billions of existing users, Google has a clear advantage. Power users wanting access to the latest versions of everything on display can now sign up for Google AI Ultra for $250 a month.\u00a0\u00a0\nWhen OpenAI released ChatGPT in late 2022, Google was caught on the back foot and was forced to jump into higher gear to catch up. With this year\u2019s product lineup, it looks as if Google has stuck its landing.\nOn a preview call, CEO Sundar Pichai claimed that AI Overviews, a precursor to AI Mode that provides LLM-generated summaries of search results, had turned out to be popular with hundreds of millions of users. He speculated that many of them may not even know (or care) that they were using AI\u2014it was just a cool new way to search. Google I/O gives a broader glimpse of that future, one where AI is invisible.\n\u201cMore intelligence is available, for everyone, everywhere,\u201d Pichai told his audience. I think we are expected to marvel. But by putting AI in everything, Google is turning AI into a technology we won\u2019t notice and may not even bother to name.",
    "content": "If you want to know where AI is headed, this year\u2019s Google I/O has you covered. The company\u2019s annual showcase of next-gen products, which kicked off yesterday, has all of the pomp and pizzazz, the sizzle reels and celebrity walk-ons, that you\u2019d expect from a multimillion-dollar marketing event.\nBut it also shows us just how fast this still experimental technology is being subsumed into a lineup designed to sell phones and subscription tiers. Never before have I seen this thing we call artificial intelligence appear so normal.\nYes, Google\u2019s roster of consumer-facing products is the slickest on offer. The firm is bundling most of its multimodal models into its Gemini app, including the new Imagen 4 image generator and the new Veo 3 video generator. That means you can now access Google\u2019s full range of generative models via a single chatbot. It also announced Gemini Live, a feature that lets you share your phone\u2019s screen or your camera\u2019s view with the chatbot and ask it about what it can see.\nThose features were previously only seen in demos of Project Astra, a \u201cuniversal AI assistant\u201c that Google DeepMind is working on. Now, Google is inching toward putting Project Astra into the hands of anyone with a smartphone.\nGoogle is also rolling out AI Mode, an LLM-powered front end to search. This can now pull in personal information from Gmail or Google Docs to tailor searches to users. It will include Deep Search, which can break a query down into hundreds of individual searches and then summarize the results; a version of Project Mariner, Google DeepMind\u2019s browser-using agent; and Search Live, which lets you hold up your camera and ask it what it sees.\nThis is the new frontier. It\u2019s no longer about who has the most powerful models, but who can spin them into the best products. OpenAI\u2019s ChatGPT includes many similar features to Gemini\u2019s. But with its existing ecosystem of consumer services and billions of existing users, Google has a clear advantage. Power users wanting access to the latest versions of everything on display can now sign up for Google AI Ultra for $250 a month.\u00a0\u00a0\nWhen OpenAI released ChatGPT in late 2022, Google was caught on the back foot and was forced to jump into higher gear to catch up. With this year\u2019s product lineup, it looks as if Google has stuck its landing.\nOn a preview call, CEO Sundar Pichai claimed that AI Overviews, a precursor to AI Mode that provides LLM-generated summaries of search results, had turned out to be popular with hundreds of millions of users. He speculated that many of them may not even know (or care) that they were using AI\u2014it was just a cool new way to search. Google I/O gives a broader glimpse of that future, one where AI is invisible.\n\u201cMore intelligence is available, for everyone, everywhere,\u201d Pichai told his audience. I think we are expected to marvel. But by putting AI in everything, Google is turning AI into a technology we won\u2019t notice and may not even bother to name."
  },
  {
    "title": "The Download: introducing the AI energy package",
    "url": "https://www.technologyreview.com/2025/05/20/1117051/the-download-introducing-the-ai-energy-package/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Tue, 20 May 2025 12:10:00 +0000",
    "published_datetime": "2025-05-20T13:10:00",
    "summary": "This is today\u2019s edition of\u00a0The Download,\u00a0our weekday newsletter that provides a daily dose of what\u2019s going on in the world of technology.\nWe did the math on AI\u2019s energy footprint. Here\u2019s the story you haven\u2019t heard.\nIt\u2019s well documented that AI is a power-hungry technology. But there has been far less reporting on the extent of that hunger, how much its appetite is set to grow in the coming years, where that power will come from, and who will pay for it.\u00a0\nFor the past six months, MIT Technology Review\u2019s team of reporters and editors have worked to answer those questions. The result is an unprecedented look at the state of AI\u2019s energy and resource usage, where it is now, where it is headed in the years to come, and why we have to get it right.\u00a0\nAt the centerpiece of this package is an entirely novel line of reporting into the demands of inference\u2014the way human beings interact with AI when we make text queries or ask AI to come up with new images or create videos. Experts say inference is set to eclipse the already massive amount of energy required to train new AI models. Here\u2019s everything we found out.\nHere\u2019s what you can expect from the rest of the package, including:\n+ We were so startled by what we learned reporting this story that we also put together a brief on everything you need to know about estimating AI\u2019s energy and emissions burden.\u00a0\n+ We went out into the world to see the effects of this energy hunger\u2014from the deserts of Nevada, where data centers in an industrial park the size of Detroit demand ever more water to keep their processors cool and running.\u00a0\n+ In Louisiana, where Meta plans its largest-ever data center, we expose the dirty secret that will fuel its AI ambitions\u2014along with those of many others.\u00a0\n+ Why the clean energy promise of powering AI data centers with nuclear energy will long remain elusive.\u00a0\n+ But it\u2019s not all doom and gloom. Check out the reasons to be optimistic, and examine why future AI systems could be far less energy intensive than today\u2019s.\n\nAI can do a better job of persuading people than we do\nThe news: Millions of people argue with each other online every day, but remarkably few of them change someone\u2019s mind. New research suggests that large language models (LLMs) might do a better job, especially when they\u2019re given the ability to adapt their arguments using personal information about individuals. The finding suggests that AI could become a powerful tool for persuading people, for better or worse.\nThe big picture: The findings are the latest in a growing body of research demonstrating LLMs\u2019 powers of persuasion. The authors warn they show how AI tools can craft sophisticated, persuasive arguments if they have even minimal information about the humans they\u2019re interacting with. Read the full story.\n\u2014Rhiannon Williams\nHow AI is introducing errors into courtrooms\nIt\u2019s been quite a couple weeks for stories about AI in the courtroom. You might have heard about the deceased victim of a road rage incident whose family created an AI avatar of him to show as an impact statement (possibly the first time this has been done in the US).But there\u2019s a bigger, far more consequential controversy brewing, legal experts say. AI hallucinations are cropping up more and more in legal filings. And it\u2019s starting to infuriate judges. Just consider these three cases, each of which gives a glimpse into what we can expect to see more of as lawyers embrace AI. Read the full story.\n\u2014James O\u2019Donnell\nThis story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.\n\nThe must-reads\nI\u2019ve combed the internet to find you today\u2019s most fun/important/scary/fascinating stories about technology.\n1 Donald Trump has signed the Take It Down Act into US lawIt criminalizes the distribution of non-consensual intimate images, including deepfakes. (The Verge)+ Tech platforms will be forced to remove such material within 48 hours of being notified. (CNN)+ It\u2019s only the sixth bill he\u2019s signed into law during his second term. (NBC News)\n2 There\u2019s now a buyer for 23andMe\u00a0Pharma firm Regeneron has swooped in and offered to help it keep operating. (WSJ $)+ The worth of your genetic data? $17. (404 Media)+ Regeneron promised to prioritize security and ethical use of that data. (TechCrunch)\n3 Microsoft is adding Elon Musk\u2019s AI models to its cloud platformErr, is that a good idea? (Bloomberg $)+ Musk wants to sell Grok to other businesses. (The Information $)\n4 Autonomous cars trained to react like humans cause fewer road injuriesA study found they were more cautious around cyclists, pedestrians and motorcyclists. (FT $)+ Waymo is expanding its robotaxi operations out of San Francisco. (Reuters)+ How Wayve\u2019s driverless cars will meet one of their biggest challenges yet. (MIT Technology Review)\n5 Hurricane season is on its wayDOGE cuts means we\u2019re less prepared. (The Atlantic $)+ COP30 may be in crisis before it\u2019s even begun. (New Scientist $)\n6 Telegram handed over data from more than 20,000 users\u00a0In the first three months of 2025 alone. (404 Media)\n7 GM has stopped exporting cars to ChinaTrump\u2019s tariffs have put an end to its export plans. (NYT $)\n8 Blended meats are on the risePlants account for up to 70% of these new meats\u2014and consumers love them. (WP $)+ Alternative meat could help the climate. Will anyone eat it? (MIT Technology Review)\n9 SAG-AFTRA isn\u2019t happy about Fornite\u2019s AI-voiced Darth VaderIt\u2019s slapped Fortnite\u2019s creators with an unfair labor practice charge. (Ars Technica)+ How Meta and AI companies recruited striking actors to train AI. (MIT Technology Review)\n10 This AI model can swiftly build Lego structuresThanks to nothing more than a prompt. (Fast Company $)\n\nQuote of the day\n\u201cPlatforms have no incentive or requirement to make sure what comes through the system is non-consensual intimate imagery.\u201d\n\u2014Becca Branum, deputy director of the Center for Democracy and Technology, says the new Take It Down Act could fuel censorship, Wired reports.\n\nOne more thing\nAre friends electric?Thankfully, the difference between humans and machines in the real world is easy to discern, at least for now. While machines tend to excel at things adults find difficult\u2014playing world-champion-level chess, say, or multiplying really big numbers\u2014they find it hard to accomplish stuff a five-year-old can do with ease, such as catching a ball or walking around a room without bumping into things.This fundamental tension\u2014what is hard for humans is easy for machines, and what\u2019s hard for machines is easy for humans\u2014is at the heart of three new books delving into our complex and often fraught relationship with robots, AI, and automation. They force us to reimagine the nature of everything from friendship and love to work, health care, and home life. Read the full story.\n\u2014Bryan Gardiner\n\nWe can still have nice things\nA place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet \u2019em at me.)\n+ Congratulations to William Goodge, who ran across Australia in just 35 days!+ A British horticulturist has created a garden at this year\u2019s Chelsea Flower Show just for dogs.+ The Netherlands just loves a sidewalk garden.+ Did you know the T Rex is a north American hero? Me neither ",
    "content": "This is today\u2019s edition of\u00a0The Download,\u00a0our weekday newsletter that provides a daily dose of what\u2019s going on in the world of technology.\nWe did the math on AI\u2019s energy footprint. Here\u2019s the story you haven\u2019t heard.\nIt\u2019s well documented that AI is a power-hungry technology. But there has been far less reporting on the extent of that hunger, how much its appetite is set to grow in the coming years, where that power will come from, and who will pay for it.\u00a0\nFor the past six months, MIT Technology Review\u2019s team of reporters and editors have worked to answer those questions. The result is an unprecedented look at the state of AI\u2019s energy and resource usage, where it is now, where it is headed in the years to come, and why we have to get it right.\u00a0\nAt the centerpiece of this package is an entirely novel line of reporting into the demands of inference\u2014the way human beings interact with AI when we make text queries or ask AI to come up with new images or create videos. Experts say inference is set to eclipse the already massive amount of energy required to train new AI models. Here\u2019s everything we found out.\nHere\u2019s what you can expect from the rest of the package, including:\n+ We were so startled by what we learned reporting this story that we also put together a brief on everything you need to know about estimating AI\u2019s energy and emissions burden.\u00a0\n+ We went out into the world to see the effects of this energy hunger\u2014from the deserts of Nevada, where data centers in an industrial park the size of Detroit demand ever more water to keep their processors cool and running.\u00a0\n+ In Louisiana, where Meta plans its largest-ever data center, we expose the dirty secret that will fuel its AI ambitions\u2014along with those of many others.\u00a0\n+ Why the clean energy promise of powering AI data centers with nuclear energy will long remain elusive.\u00a0\n+ But it\u2019s not all doom and gloom. Check out the reasons to be optimistic, and examine why future AI systems could be far less energy intensive than today\u2019s.\n\nAI can do a better job of persuading people than we do\nThe news: Millions of people argue with each other online every day, but remarkably few of them change someone\u2019s mind. New research suggests that large language models (LLMs) might do a better job, especially when they\u2019re given the ability to adapt their arguments using personal information about individuals. The finding suggests that AI could become a powerful tool for persuading people, for better or worse.\nThe big picture: The findings are the latest in a growing body of research demonstrating LLMs\u2019 powers of persuasion. The authors warn they show how AI tools can craft sophisticated, persuasive arguments if they have even minimal information about the humans they\u2019re interacting with. Read the full story.\n\u2014Rhiannon Williams\nHow AI is introducing errors into courtrooms\nIt\u2019s been quite a couple weeks for stories about AI in the courtroom. You might have heard about the deceased victim of a road rage incident whose family created an AI avatar of him to show as an impact statement (possibly the first time this has been done in the US).But there\u2019s a bigger, far more consequential controversy brewing, legal experts say. AI hallucinations are cropping up more and more in legal filings. And it\u2019s starting to infuriate judges. Just consider these three cases, each of which gives a glimpse into what we can expect to see more of as lawyers embrace AI. Read the full story.\n\u2014James O\u2019Donnell\nThis story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.\n\nThe must-reads\nI\u2019ve combed the internet to find you today\u2019s most fun/important/scary/fascinating stories about technology.\n1 Donald Trump has signed the Take It Down Act into US lawIt criminalizes the distribution of non-consensual intimate images, including deepfakes. (The Verge)+ Tech platforms will be forced to remove such material within 48 hours of being notified. (CNN)+ It\u2019s only the sixth bill he\u2019s signed into law during his second term. (NBC News)\n2 There\u2019s now a buyer for 23andMe\u00a0Pharma firm Regeneron has swooped in and offered to help it keep operating. (WSJ $)+ The worth of your genetic data? $17. (404 Media)+ Regeneron promised to prioritize security and ethical use of that data. (TechCrunch)\n3 Microsoft is adding Elon Musk\u2019s AI models to its cloud platformErr, is that a good idea? (Bloomberg $)+ Musk wants to sell Grok to other businesses. (The Information $)\n4 Autonomous cars trained to react like humans cause fewer road injuriesA study found they were more cautious around cyclists, pedestrians and motorcyclists. (FT $)+ Waymo is expanding its robotaxi operations out of San Francisco. (Reuters)+ How Wayve\u2019s driverless cars will meet one of their biggest challenges yet. (MIT Technology Review)\n5 Hurricane season is on its wayDOGE cuts means we\u2019re less prepared. (The Atlantic $)+ COP30 may be in crisis before it\u2019s even begun. (New Scientist $)\n6 Telegram handed over data from more than 20,000 users\u00a0In the first three months of 2025 alone. (404 Media)\n7 GM has stopped exporting cars to ChinaTrump\u2019s tariffs have put an end to its export plans. (NYT $)\n8 Blended meats are on the risePlants account for up to 70% of these new meats\u2014and consumers love them. (WP $)+ Alternative meat could help the climate. Will anyone eat it? (MIT Technology Review)\n9 SAG-AFTRA isn\u2019t happy about Fornite\u2019s AI-voiced Darth VaderIt\u2019s slapped Fortnite\u2019s creators with an unfair labor practice charge. (Ars Technica)+ How Meta and AI companies recruited striking actors to train AI. (MIT Technology Review)\n10 This AI model can swiftly build Lego structuresThanks to nothing more than a prompt. (Fast Company $)\n\nQuote of the day\n\u201cPlatforms have no incentive or requirement to make sure what comes through the system is non-consensual intimate imagery.\u201d\n\u2014Becca Branum, deputy director of the Center for Democracy and Technology, says the new Take It Down Act could fuel censorship, Wired reports.\n\nOne more thing\nAre friends electric?Thankfully, the difference between humans and machines in the real world is easy to discern, at least for now. While machines tend to excel at things adults find difficult\u2014playing world-champion-level chess, say, or multiplying really big numbers\u2014they find it hard to accomplish stuff a five-year-old can do with ease, such as catching a ball or walking around a room without bumping into things.This fundamental tension\u2014what is hard for humans is easy for machines, and what\u2019s hard for machines is easy for humans\u2014is at the heart of three new books delving into our complex and often fraught relationship with robots, AI, and automation. They force us to reimagine the nature of everything from friendship and love to work, health care, and home life. Read the full story.\n\u2014Bryan Gardiner\n\nWe can still have nice things\nA place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet \u2019em at me.)\n+ Congratulations to William Goodge, who ran across Australia in just 35 days!+ A British horticulturist has created a garden at this year\u2019s Chelsea Flower Show just for dogs.+ The Netherlands just loves a sidewalk garden.+ Did you know the T Rex is a north American hero? Me neither "
  },
  {
    "title": "AI could keep us dependent on natural gas for decades to come",
    "url": "https://www.technologyreview.com/2025/05/20/1116272/ai-natural-gas-data-centers-energy-power-plants/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Tue, 20 May 2025 09:00:00 +0000",
    "published_datetime": "2025-05-20T10:00:00",
    "summary": "The thousands of sprawling acres in rural northeast Louisiana had gone unwanted for nearly two decades. Louisiana authorities bought the land in Richland Parish in 2006 to promote economic development in one of the poorest regions in the state. For years, they marketed the former agricultural fields as the Franklin Farm mega site, first to auto manufacturers (no takers) and after that to other industries that might want to occupy more than a thousand\u00a0acres just off the interstate.\n\n\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\n\n\nSo it\u2019s no wonder that state and local politicians were exuberant when Meta showed up. In December, the company announced plans to build a massive $10 billion data center for training its artificial-intelligence models at the site, with operations to begin in 2028. \u201cA game changer,\u201d declared Governor Jeff Landry, citing 5,000 construction jobs and 500 jobs at the data center that are expected to be created and calling it the largest private capital investment in the state\u2019s history. From a rural backwater to the heart of the booming AI revolution!\nThe AI data center also promises to transform the state\u2019s energy future. Stretching in length for more than a mile, it will be Meta\u2019s largest in the world, and it will have an enormous appetite for electricity, requiring two gigawatts for computation alone (the electricity for cooling and other building needs will add to that). When it\u2019s up and running, it will be the equivalent of suddenly adding a decent-size city to the region\u2019s grid\u2014one that never sleeps and needs a steady, uninterrupted flow of electricity.\nTo power the data center, Entergy aims to spend $3.2 billion to build three large natural-gas power plants with a total capacity of 2.3 gigawatts and upgrade the grid to accommodate the huge jump in anticipated demand. In its filing to the state\u2019s power regulatory agency, Entergy acknowledged that natural-gas plants \u201cemit significant amounts of CO2\u201d but said the\u00a0energy source was the only affordable choice given the need to quickly meet the 24-7 electricity demand from the huge data center.\nMeta said it will work with Entergy to eventually bring online at least 1.5 gigawatts of new renewables, including solar, but that it had not yet decided which specific projects to fund or when those investments will be made. Meanwhile, the new natural-gas plants, which are scheduled to be up and running starting in 2028 and will have a typical lifetime of around 30 years, will further lock in the state\u2019s commitment to the fossil fuel. \nThe development has sparked interest from the US Congress;\u00a0last week, Sheldon Whitehouse, the ranking member of the Senate Committee on Environment and Public Works issued a letter to Meta that called out the company\u2019s plan to power its data center with \u201cnew and unabated natural gas generation\u201d and said its promises to offset the resulting emissions \u201cby funding carbon capture and a solar project are vague and offer little reassurance.\u201d\nThe choice of natural gas as the go-to solution to meet the growing demand for power from AI is not unique to Louisiana. The fossil fuel is already the country\u2019s chief source of electricity generation, and large natural-gas plants are being built around the country to feed electricity to new and planned AI data centers. While some climate advocates have hoped that cleaner renewable power would soon overtake it, the booming power demand from data centers is all but wiping out any prospect that the US will wean itself off natural gas anytime soon.\nThe reality on the ground is that natural gas is \u201cthe default\u201d to meet the exploding power demand from AI data centers, says David Victor, a political scientist at the University of California, San Diego, and co-director of its Deep Decarbonization Project. \u201cThe natural-gas plant is the thing that you know how to build, you know what it\u2019s going to cost (more or less), and you know how to scale it and get it approved,\u201d says Victor. \u201cEven for [AI] companies that want to have low emissions profiles and who are big pushers of low or zero carbon, they won\u2019t have a choice but to use gas.\u201d\nThe preference for natural gas is particularly pronounced in the American South, where plans for multiple large gas-fired plants are in the works in states such as Virginia, North Carolina, South Carolina, and Georgia. Utilities in those states alone are planning some 20 gigawatts of new natural-gas power plants over the next 15 years, according to a recent report. And much of the new demand\u2014particularly in Virginia, South Carolina and Georgia\u2014is coming from data centers; in those 3 states data centers account for around 65 to 85% of projected load growth.\n\u201cIt\u2019s a long-term commitment in absolutely the wrong direction,\u201d says Greg Buppert, a senior attorney at the Southern Environmental Law Center in Charlottesville, Virginia. If all the proposed gas plants get built in the South over the next 15 years, he says, \u201cwe\u2019ll just have to accept that we won\u2019t meet emissions reduction goals.\u201d\nBut even as it looks more and more likely that natural gas will remain a sizable part of our energy future, questions abound over just what its continued dominance will look like.\nFor one thing, no one is sure exactly how much electricity AI data centers will need in the future and how large an appetite companies will have for natural gas. Demand for AI could fizzle. Or AI companies could make a concerted effort to shift to renewable energy or nuclear power. Such possibilities mean that the US could be on a path to overbuild natural-gas capacity, which would leave regions saddled with unneeded and polluting fossil-fuel dinosaurs\u2014and residents footing soaring electricity bills to pay off today\u2019s investments.\nThe good news is that such risks could likely be managed over the next few years, if\u2014and it\u2019s a big if\u2014AI companies are more transparent about how flexible they can be in their seemingly insatiable energy demands.\nThe reign of natural gas\nNatural gas in the US is cheap and abundant these days. Two decades ago, huge reserves were found in shale deposits scattered across the country. In 2008, as fracking started to make it possible to extract large quantities of the gas from shale, natural gas was selling for $13 per million Btu (a measure of thermal energy); last year, it averaged just $2.21, the lowest annual price (adjusting for inflation) ever reported, according to the US Energy Information Administration (EIA).\nAround 2016, natural gas overtook coal as the main fuel for electricity generation in the US. And today\u2014despite the rapid rise of solar and wind power, and well-deserved enthusiasm for the falling price of such renewables\u2014natural gas is still king, accounting for around 40% of electricity generated in the US. In Louisiana, which is also a big producer, that share is some 72%, according to a recent audit.\nNatural gas burns much cleaner than coal, producing roughly half as much carbon dioxide. In the early days of the gas revolution, many environmental activists and progressive politicians touted it as a valuable \u201cbridge\u201d to renewables and other sources of clean energy. And by some calculations, natural gas has fulfilled that promise. The power sector has been one of the few success stories in lowering US emissions, thanks to its use of natural gas as a replacement for coal.\u00a0\u00a0\n\nBut natural gas still produces a lot of carbon dioxide when it is burned in conventionally equipped power plants. And fracking causes local air and water pollution. Perhaps most worrisome, drilling and pipelines are releasing substantial amounts of methane, the main ingredient in natural gas, both accidentally and by intentional venting. Methane is a far more potent greenhouse gas than carbon dioxide, and the emissions are a growing concern to climate scientists, albeit one that\u2019s difficult to quantify.\nStill, carbon emissions from the power sector will likely continue to drop as coal is further squeezed out and more renewables get built, according to the Rhodium Group, a research consultancy. But Rhodium also projects that if electricity demand from data centers remains high and natural-gas prices low, the fossil fuel will remain the dominant source of power generation at least through 2035 and the transition to cleaner electricity will be much delayed. Rhodium estimates that the continued reign of natural gas will lead to an additional 278 million metric tons of annual US carbon emissions by 2035 (roughly equivalent to the emissions from a large US state such as Florida), relative to a future in which the use of fossil fuel gradually winds down.\nOur addiction to natural gas, however, doesn\u2019t have to be a total climate disaster, at least over the longer term. Large AI companies could use their vast leverage to insist that utilities install carbon capture and sequestration (CCS) at power plants and use natural gas sourced with limited methane emissions.\nEntergy, for one, says its new gas turbines will be able to incorporate CCS through future upgrades. And Meta says it will help to fund the installation of CCS equipment at one of Entergy\u2019s existing natural-gas power plants in southern Louisiana to help prove out the technology.\u00a0\u00a0\nBut the transition to clean natural gas is a hope that will take decades to realize. Meanwhile, utilities across the country are facing a more imminent and practical challenge: how to meet the sudden demand for gigawatts more power in the next few years without inadvertently building far too much capacity. For many, adding more natural-gas power plants might seem like the safe bet. But what if the explosion in AI demand doesn\u2019t show up?\nTimes of stress\nAI companies tout the need for massive, power-hungry data centers. But estimates for just how much energy it will actually take to train and run AI models vary wildly. And the technology keeps changing, sometimes seemingly overnight. DeepSeek, the new Chinese model that debuted in January, may or may not signal a future of new energy-efficient AI, but it certainly raises the possibility that such advances are possible. Maybe we will find ways to use far more energy-efficient hardware. Or maybe the AI revolution will peter out and many of the massive data centers that companies think they\u2019ll need will never get built. There are already signs that too many have been constructed in China and clues that it might be beginning to happen in the US.\u00a0\nDespite the uncertainty, power providers have the task of drawing up long-term plans for investments to accommodate projected demand. Too little capacity and their customers face blackouts; too much and those customers face outsize electricity bills to fund investments in unneeded power.\nThere could be a way to lessen the risk of overbuilding natural-gas power, however. Plenty of power is available on average around the country and on most regional grids. Most utilities typically use only about 53% of their available capacity on average during the year, according to a Duke study. The problem is that utilities must be prepared for the few hours when demand spikes\u2014say, because of severe winter weather or a summer heat wave.\nThe soaring demand from AI data centers is prompting many power providers to plan new capacity to make sure they have plenty of what Tyler Norris, a fellow at Duke\u2019s Nicholas School of the Environment, and his colleagues call \u201cheadroom,\u201d to meet any spikes in demand. But after analyzing data from power systems across the country, Norris and his coauthors found that if large AI facilities cut back their electricity use during hours of peak demand, many regional power grids could accommodate those AI customers without adding new generation capacity.\nEven a moderate level of flexibility would make a huge difference. The Duke researchers estimate that if data centers cut their electricity use by roughly half for just a few hours during the year, it will allow utilities to handle some additional 76 gigawatts of new demand. That means power providers could effectively absorb the 65 or so additional gigawatts that, according to some predictions, data centers will likely need by 2029.\n\u201cThe prevailing assumption is that data centers are 100% inflexible,\u201d says Norris. That is, that they need to run at full power all the time. But Norris says AI data centers, particularly ones that are training large foundation models (such as Meta\u2019s facility in Richland Parish), can avoid running at full capacity or shift their computation loads to other data centers around the country\u2014or even ramp up their own backup power\u2014during times when a grid is under stress.\nThe increased flexibility could allow companies to get AI data centers up and running faster, without waiting for new power plants and upgrades to transmission lines\u2014which can take years to get approved and built. It could also, Norris noted in testimony to the US Congress in early March, provide at least a short-term reprieve on the rush to build more natural-gas power, buying time for utilities to develop and plan for cleaner technologies such as advanced nuclear and enhanced geothermal. It could, he testified, prevent \u201ca hasty overbuild of natural-gas infrastructure.\u201d\nAI companies have expressed some interest in their ability to shift around demand for power. But there are still plenty of technology questions around how to make it happen. Late last year, EPRI (the Electric Power Research Institute), a nonprofit R&D group, started a three-year collaboration with power providers, grid operators, and AI companies including Meta and Google, to figure it out. \u201cThe potential is very large,\u201d says David Porter, the EPRI vice president who runs the project, but we must show it works \u201cbeyond just something on a piece of paper or a computer screen.\u201d\nPorter estimates that there are typically 80 to 90 hours a year when a local grid is under stress and it would help for a data center to reduce its energy use. But, he says, AI data centers still need to figure out how to throttle back at those times, and grid operators need to learn how to suddenly subtract and then add back hundreds of megawatts of electricity without disrupting their systems. \u201cThere\u2019s still a lot of work to be done so that it\u2019s seamless for the continuous operation of the data centers and seamless for the continuous operation of the grid,\u201d he says.\nFooting the bill\nUltimately, getting AI data centers to be more flexible in their power demands will require more than a technological fix. It will require a shift in how AI companies work with utilities and local communities, providing them with more information and insights into actual electricity needs. And it will take aggressive regulators to make sure utilities are rigorously evaluating the power requirements of data centers rather than just reflexively building more natural-gas plants.\n\u201cThe most important climate policymakers in the country right now are not in Washington. They\u2019re in state capitals, and these are public utility commissioners,\u201d says Costa Samaras, the director of Carnegie Mellon University\u2019s Scott Institute for Energy Innovation.\nIn Louisiana, those policymakers are the elected officials at the Louisiana Public Service Commission, who are expected to rule later this year on Entergy\u2019s proposed new gas plants and grid upgrades. The LPSC commissioners will decide whether Entergy\u2019s arguments about the huge energy requirements of Meta\u2019s data center and need for full 24/7 power leave no alternative to natural gas.\u00a0\nIn the application it filed last fall with LPSC, Entergy said natural-gas power was essential for it to meet demand \u201cthroughout the day and night.\u201d Teaming up solar power with battery storage could work \u201cin theory\u201d but would be \u201cprohibitively costly.\u201d Entergy also ruled out nuclear, saying it would take too long and cost too much.\nOthers are not satisfied with the utility\u2019s judgment. In February, the New Orleans\u2013based Alliance for Affordable Energy and the Union of Concerned Scientists filed a motion with the Louisiana regulators arguing that Entergy did not do a rigorous market evaluation of its options, as required by the commission\u2019s rules. Part of the problem, the groups said, is that Entergy relied on \u201cunsubstantiated assertions\u201d from Meta on its load needs and timeline.\n\u201cEntergy is saying [Meta] needs around-the-clock power,\u201d says Paul Arbaje, an analyst for the climate and energy program at the Union of Concerned Scientists. \u201cBut we\u2019re just being asked to take [Entergy\u2019s] word for it. Regulators need to be asking tough questions and not just assume that these data centers need to be operated at essentially full capacity all the time.\u201d And, he suggests, if the utility had \u201cstarted to poke holes at the assumptions that are sometimes taken as a given,\u201d it \u201cwould have found other cleaner options.\u201d\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nIn an email response to MIT Technology Review, Entergy said that it has discussed the operational aspects of the facility with Meta, but \u201cas with all customers, Entergy Louisiana will not discuss sensitive matters on behalf of their customers.\u201d In a letter filed with the state\u2019s regulators in early April, Meta said Entergy\u2019s understanding of its energy needs is, in fact, accurate.\nThe February motion also raised concern over who will end up paying for the new gas plants. Entergy says Meta has signed a 15-year supply contract for the electricity that is meant to help cover the costs of building and running the power plants but didn\u2019t respond to requests by MIT Technology Review for further details of the deal, including what happens if Meta wants to terminate the contract early.\nMeta referred MIT Technology Review\u2019s questions about the contract to Entergy but says its policy is to cover the full cost that utilities incur to serve its data centers, including grid upgrades. It also says it is spending over $200 million to support the Richland Parish data centers with new infrastructure, including roads and water systems.\u00a0\nNot everyone is convinced. The Alliance for Affordable Energy, which works on behalf of Louisiana residents, says that the large investments in new gas turbines could mean future rate hikes, in a state where residents already have high electricity bills and suffer from one of country\u2019s most unreliable grids. Of special concern is what happens after the 15 years. \n\u201cOur biggest long-term concern is that in 15 years, residential ratepayers [and] small businesses in Louisiana will be left holding the bag for three large gas generators,\u201d says Logan Burke, the alliance\u2019s executive director.\nIndeed, consumers across the country have good reasons to fear that their electricity bills will go up as utilities look to meet the increased demand from AI data centers by building new generation capacity. In a paper posted in March, researchers at Harvard Law School argued that utilities \u201care now forcing the public to pay for infrastructure designed to supply a handful of exceedingly wealthy corporations.\u201d\nThe Harvard authors write, \u201cUtilities tell [public utility commissions] what they want to hear: that the deals for Big Tech isolate data center energy costs from other ratepayers\u2019 bills and won\u2019t increase consumers\u2019 power prices.\u201d But the complexity of the utilities\u2019 payment data and lack of transparency in the accounting, they say, make verifying this claim \u201call but impossible.\u201d\nThe boom in AI data centers is making Big Tech a player in our energy infrastructure and electricity future in a way unimaginable just a few years ago. At their best, AI companies could greatly facilitate the move to cleaner energy by acting as reliable and well-paying customers that provide funding that utilities can use to invest in a more robust and flexible electricity grid. This change can happen without burdening other electricity customers with additional risks and costs. But it will take AI companies committed to that vision. And it will take state regulators who ask tough questions and don\u2019t get carried away by the potential investments being dangled by AI companies.\nHuge new AI data centers like the one in Richland Parish could in fact be a huge economic boon by providing new jobs, but residents deserve transparency and input into the negotiations. This is, after all, public infrastructure. Meta may come and go, but Louisiana\u2019s residents will have to live with\u2014and possibly pay for\u2014the changes in the decades to come.",
    "content": "The thousands of sprawling acres in rural northeast Louisiana had gone unwanted for nearly two decades. Louisiana authorities bought the land in Richland Parish in 2006 to promote economic development in one of the poorest regions in the state. For years, they marketed the former agricultural fields as the Franklin Farm mega site, first to auto manufacturers (no takers) and after that to other industries that might want to occupy more than a thousand\u00a0acres just off the interstate.\n\n\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\n\n\nSo it\u2019s no wonder that state and local politicians were exuberant when Meta showed up. In December, the company announced plans to build a massive $10 billion data center for training its artificial-intelligence models at the site, with operations to begin in 2028. \u201cA game changer,\u201d declared Governor Jeff Landry, citing 5,000 construction jobs and 500 jobs at the data center that are expected to be created and calling it the largest private capital investment in the state\u2019s history. From a rural backwater to the heart of the booming AI revolution!\nThe AI data center also promises to transform the state\u2019s energy future. Stretching in length for more than a mile, it will be Meta\u2019s largest in the world, and it will have an enormous appetite for electricity, requiring two gigawatts for computation alone (the electricity for cooling and other building needs will add to that). When it\u2019s up and running, it will be the equivalent of suddenly adding a decent-size city to the region\u2019s grid\u2014one that never sleeps and needs a steady, uninterrupted flow of electricity.\nTo power the data center, Entergy aims to spend $3.2 billion to build three large natural-gas power plants with a total capacity of 2.3 gigawatts and upgrade the grid to accommodate the huge jump in anticipated demand. In its filing to the state\u2019s power regulatory agency, Entergy acknowledged that natural-gas plants \u201cemit significant amounts of CO2\u201d but said the\u00a0energy source was the only affordable choice given the need to quickly meet the 24-7 electricity demand from the huge data center.\nMeta said it will work with Entergy to eventually bring online at least 1.5 gigawatts of new renewables, including solar, but that it had not yet decided which specific projects to fund or when those investments will be made. Meanwhile, the new natural-gas plants, which are scheduled to be up and running starting in 2028 and will have a typical lifetime of around 30 years, will further lock in the state\u2019s commitment to the fossil fuel. \nThe development has sparked interest from the US Congress;\u00a0last week, Sheldon Whitehouse, the ranking member of the Senate Committee on Environment and Public Works issued a letter to Meta that called out the company\u2019s plan to power its data center with \u201cnew and unabated natural gas generation\u201d and said its promises to offset the resulting emissions \u201cby funding carbon capture and a solar project are vague and offer little reassurance.\u201d\nThe choice of natural gas as the go-to solution to meet the growing demand for power from AI is not unique to Louisiana. The fossil fuel is already the country\u2019s chief source of electricity generation, and large natural-gas plants are being built around the country to feed electricity to new and planned AI data centers. While some climate advocates have hoped that cleaner renewable power would soon overtake it, the booming power demand from data centers is all but wiping out any prospect that the US will wean itself off natural gas anytime soon.\nThe reality on the ground is that natural gas is \u201cthe default\u201d to meet the exploding power demand from AI data centers, says David Victor, a political scientist at the University of California, San Diego, and co-director of its Deep Decarbonization Project. \u201cThe natural-gas plant is the thing that you know how to build, you know what it\u2019s going to cost (more or less), and you know how to scale it and get it approved,\u201d says Victor. \u201cEven for [AI] companies that want to have low emissions profiles and who are big pushers of low or zero carbon, they won\u2019t have a choice but to use gas.\u201d\nThe preference for natural gas is particularly pronounced in the American South, where plans for multiple large gas-fired plants are in the works in states such as Virginia, North Carolina, South Carolina, and Georgia. Utilities in those states alone are planning some 20 gigawatts of new natural-gas power plants over the next 15 years, according to a recent report. And much of the new demand\u2014particularly in Virginia, South Carolina and Georgia\u2014is coming from data centers; in those 3 states data centers account for around 65 to 85% of projected load growth.\n\u201cIt\u2019s a long-term commitment in absolutely the wrong direction,\u201d says Greg Buppert, a senior attorney at the Southern Environmental Law Center in Charlottesville, Virginia. If all the proposed gas plants get built in the South over the next 15 years, he says, \u201cwe\u2019ll just have to accept that we won\u2019t meet emissions reduction goals.\u201d\nBut even as it looks more and more likely that natural gas will remain a sizable part of our energy future, questions abound over just what its continued dominance will look like.\nFor one thing, no one is sure exactly how much electricity AI data centers will need in the future and how large an appetite companies will have for natural gas. Demand for AI could fizzle. Or AI companies could make a concerted effort to shift to renewable energy or nuclear power. Such possibilities mean that the US could be on a path to overbuild natural-gas capacity, which would leave regions saddled with unneeded and polluting fossil-fuel dinosaurs\u2014and residents footing soaring electricity bills to pay off today\u2019s investments.\nThe good news is that such risks could likely be managed over the next few years, if\u2014and it\u2019s a big if\u2014AI companies are more transparent about how flexible they can be in their seemingly insatiable energy demands.\nThe reign of natural gas\nNatural gas in the US is cheap and abundant these days. Two decades ago, huge reserves were found in shale deposits scattered across the country. In 2008, as fracking started to make it possible to extract large quantities of the gas from shale, natural gas was selling for $13 per million Btu (a measure of thermal energy); last year, it averaged just $2.21, the lowest annual price (adjusting for inflation) ever reported, according to the US Energy Information Administration (EIA).\nAround 2016, natural gas overtook coal as the main fuel for electricity generation in the US. And today\u2014despite the rapid rise of solar and wind power, and well-deserved enthusiasm for the falling price of such renewables\u2014natural gas is still king, accounting for around 40% of electricity generated in the US. In Louisiana, which is also a big producer, that share is some 72%, according to a recent audit.\nNatural gas burns much cleaner than coal, producing roughly half as much carbon dioxide. In the early days of the gas revolution, many environmental activists and progressive politicians touted it as a valuable \u201cbridge\u201d to renewables and other sources of clean energy. And by some calculations, natural gas has fulfilled that promise. The power sector has been one of the few success stories in lowering US emissions, thanks to its use of natural gas as a replacement for coal.\u00a0\u00a0\n\nBut natural gas still produces a lot of carbon dioxide when it is burned in conventionally equipped power plants. And fracking causes local air and water pollution. Perhaps most worrisome, drilling and pipelines are releasing substantial amounts of methane, the main ingredient in natural gas, both accidentally and by intentional venting. Methane is a far more potent greenhouse gas than carbon dioxide, and the emissions are a growing concern to climate scientists, albeit one that\u2019s difficult to quantify.\nStill, carbon emissions from the power sector will likely continue to drop as coal is further squeezed out and more renewables get built, according to the Rhodium Group, a research consultancy. But Rhodium also projects that if electricity demand from data centers remains high and natural-gas prices low, the fossil fuel will remain the dominant source of power generation at least through 2035 and the transition to cleaner electricity will be much delayed. Rhodium estimates that the continued reign of natural gas will lead to an additional 278 million metric tons of annual US carbon emissions by 2035 (roughly equivalent to the emissions from a large US state such as Florida), relative to a future in which the use of fossil fuel gradually winds down.\nOur addiction to natural gas, however, doesn\u2019t have to be a total climate disaster, at least over the longer term. Large AI companies could use their vast leverage to insist that utilities install carbon capture and sequestration (CCS) at power plants and use natural gas sourced with limited methane emissions.\nEntergy, for one, says its new gas turbines will be able to incorporate CCS through future upgrades. And Meta says it will help to fund the installation of CCS equipment at one of Entergy\u2019s existing natural-gas power plants in southern Louisiana to help prove out the technology.\u00a0\u00a0\nBut the transition to clean natural gas is a hope that will take decades to realize. Meanwhile, utilities across the country are facing a more imminent and practical challenge: how to meet the sudden demand for gigawatts more power in the next few years without inadvertently building far too much capacity. For many, adding more natural-gas power plants might seem like the safe bet. But what if the explosion in AI demand doesn\u2019t show up?\nTimes of stress\nAI companies tout the need for massive, power-hungry data centers. But estimates for just how much energy it will actually take to train and run AI models vary wildly. And the technology keeps changing, sometimes seemingly overnight. DeepSeek, the new Chinese model that debuted in January, may or may not signal a future of new energy-efficient AI, but it certainly raises the possibility that such advances are possible. Maybe we will find ways to use far more energy-efficient hardware. Or maybe the AI revolution will peter out and many of the massive data centers that companies think they\u2019ll need will never get built. There are already signs that too many have been constructed in China and clues that it might be beginning to happen in the US.\u00a0\nDespite the uncertainty, power providers have the task of drawing up long-term plans for investments to accommodate projected demand. Too little capacity and their customers face blackouts; too much and those customers face outsize electricity bills to fund investments in unneeded power.\nThere could be a way to lessen the risk of overbuilding natural-gas power, however. Plenty of power is available on average around the country and on most regional grids. Most utilities typically use only about 53% of their available capacity on average during the year, according to a Duke study. The problem is that utilities must be prepared for the few hours when demand spikes\u2014say, because of severe winter weather or a summer heat wave.\nThe soaring demand from AI data centers is prompting many power providers to plan new capacity to make sure they have plenty of what Tyler Norris, a fellow at Duke\u2019s Nicholas School of the Environment, and his colleagues call \u201cheadroom,\u201d to meet any spikes in demand. But after analyzing data from power systems across the country, Norris and his coauthors found that if large AI facilities cut back their electricity use during hours of peak demand, many regional power grids could accommodate those AI customers without adding new generation capacity.\nEven a moderate level of flexibility would make a huge difference. The Duke researchers estimate that if data centers cut their electricity use by roughly half for just a few hours during the year, it will allow utilities to handle some additional 76 gigawatts of new demand. That means power providers could effectively absorb the 65 or so additional gigawatts that, according to some predictions, data centers will likely need by 2029.\n\u201cThe prevailing assumption is that data centers are 100% inflexible,\u201d says Norris. That is, that they need to run at full power all the time. But Norris says AI data centers, particularly ones that are training large foundation models (such as Meta\u2019s facility in Richland Parish), can avoid running at full capacity or shift their computation loads to other data centers around the country\u2014or even ramp up their own backup power\u2014during times when a grid is under stress.\nThe increased flexibility could allow companies to get AI data centers up and running faster, without waiting for new power plants and upgrades to transmission lines\u2014which can take years to get approved and built. It could also, Norris noted in testimony to the US Congress in early March, provide at least a short-term reprieve on the rush to build more natural-gas power, buying time for utilities to develop and plan for cleaner technologies such as advanced nuclear and enhanced geothermal. It could, he testified, prevent \u201ca hasty overbuild of natural-gas infrastructure.\u201d\nAI companies have expressed some interest in their ability to shift around demand for power. But there are still plenty of technology questions around how to make it happen. Late last year, EPRI (the Electric Power Research Institute), a nonprofit R&D group, started a three-year collaboration with power providers, grid operators, and AI companies including Meta and Google, to figure it out. \u201cThe potential is very large,\u201d says David Porter, the EPRI vice president who runs the project, but we must show it works \u201cbeyond just something on a piece of paper or a computer screen.\u201d\nPorter estimates that there are typically 80 to 90 hours a year when a local grid is under stress and it would help for a data center to reduce its energy use. But, he says, AI data centers still need to figure out how to throttle back at those times, and grid operators need to learn how to suddenly subtract and then add back hundreds of megawatts of electricity without disrupting their systems. \u201cThere\u2019s still a lot of work to be done so that it\u2019s seamless for the continuous operation of the data centers and seamless for the continuous operation of the grid,\u201d he says.\nFooting the bill\nUltimately, getting AI data centers to be more flexible in their power demands will require more than a technological fix. It will require a shift in how AI companies work with utilities and local communities, providing them with more information and insights into actual electricity needs. And it will take aggressive regulators to make sure utilities are rigorously evaluating the power requirements of data centers rather than just reflexively building more natural-gas plants.\n\u201cThe most important climate policymakers in the country right now are not in Washington. They\u2019re in state capitals, and these are public utility commissioners,\u201d says Costa Samaras, the director of Carnegie Mellon University\u2019s Scott Institute for Energy Innovation.\nIn Louisiana, those policymakers are the elected officials at the Louisiana Public Service Commission, who are expected to rule later this year on Entergy\u2019s proposed new gas plants and grid upgrades. The LPSC commissioners will decide whether Entergy\u2019s arguments about the huge energy requirements of Meta\u2019s data center and need for full 24/7 power leave no alternative to natural gas.\u00a0\nIn the application it filed last fall with LPSC, Entergy said natural-gas power was essential for it to meet demand \u201cthroughout the day and night.\u201d Teaming up solar power with battery storage could work \u201cin theory\u201d but would be \u201cprohibitively costly.\u201d Entergy also ruled out nuclear, saying it would take too long and cost too much.\nOthers are not satisfied with the utility\u2019s judgment. In February, the New Orleans\u2013based Alliance for Affordable Energy and the Union of Concerned Scientists filed a motion with the Louisiana regulators arguing that Entergy did not do a rigorous market evaluation of its options, as required by the commission\u2019s rules. Part of the problem, the groups said, is that Entergy relied on \u201cunsubstantiated assertions\u201d from Meta on its load needs and timeline.\n\u201cEntergy is saying [Meta] needs around-the-clock power,\u201d says Paul Arbaje, an analyst for the climate and energy program at the Union of Concerned Scientists. \u201cBut we\u2019re just being asked to take [Entergy\u2019s] word for it. Regulators need to be asking tough questions and not just assume that these data centers need to be operated at essentially full capacity all the time.\u201d And, he suggests, if the utility had \u201cstarted to poke holes at the assumptions that are sometimes taken as a given,\u201d it \u201cwould have found other cleaner options.\u201d\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nIn an email response to MIT Technology Review, Entergy said that it has discussed the operational aspects of the facility with Meta, but \u201cas with all customers, Entergy Louisiana will not discuss sensitive matters on behalf of their customers.\u201d In a letter filed with the state\u2019s regulators in early April, Meta said Entergy\u2019s understanding of its energy needs is, in fact, accurate.\nThe February motion also raised concern over who will end up paying for the new gas plants. Entergy says Meta has signed a 15-year supply contract for the electricity that is meant to help cover the costs of building and running the power plants but didn\u2019t respond to requests by MIT Technology Review for further details of the deal, including what happens if Meta wants to terminate the contract early.\nMeta referred MIT Technology Review\u2019s questions about the contract to Entergy but says its policy is to cover the full cost that utilities incur to serve its data centers, including grid upgrades. It also says it is spending over $200 million to support the Richland Parish data centers with new infrastructure, including roads and water systems.\u00a0\nNot everyone is convinced. The Alliance for Affordable Energy, which works on behalf of Louisiana residents, says that the large investments in new gas turbines could mean future rate hikes, in a state where residents already have high electricity bills and suffer from one of country\u2019s most unreliable grids. Of special concern is what happens after the 15 years. \n\u201cOur biggest long-term concern is that in 15 years, residential ratepayers [and] small businesses in Louisiana will be left holding the bag for three large gas generators,\u201d says Logan Burke, the alliance\u2019s executive director.\nIndeed, consumers across the country have good reasons to fear that their electricity bills will go up as utilities look to meet the increased demand from AI data centers by building new generation capacity. In a paper posted in March, researchers at Harvard Law School argued that utilities \u201care now forcing the public to pay for infrastructure designed to supply a handful of exceedingly wealthy corporations.\u201d\nThe Harvard authors write, \u201cUtilities tell [public utility commissions] what they want to hear: that the deals for Big Tech isolate data center energy costs from other ratepayers\u2019 bills and won\u2019t increase consumers\u2019 power prices.\u201d But the complexity of the utilities\u2019 payment data and lack of transparency in the accounting, they say, make verifying this claim \u201call but impossible.\u201d\nThe boom in AI data centers is making Big Tech a player in our energy infrastructure and electricity future in a way unimaginable just a few years ago. At their best, AI companies could greatly facilitate the move to cleaner energy by acting as reliable and well-paying customers that provide funding that utilities can use to invest in a more robust and flexible electricity grid. This change can happen without burdening other electricity customers with additional risks and costs. But it will take AI companies committed to that vision. And it will take state regulators who ask tough questions and don\u2019t get carried away by the potential investments being dangled by AI companies.\nHuge new AI data centers like the one in Richland Parish could in fact be a huge economic boon by providing new jobs, but residents deserve transparency and input into the negotiations. This is, after all, public infrastructure. Meta may come and go, but Louisiana\u2019s residents will have to live with\u2014and possibly pay for\u2014the changes in the decades to come."
  },
  {
    "title": "AI\u2019s energy impact is still small\u2014but how we handle it is huge",
    "url": "https://www.technologyreview.com/2025/05/20/1116274/opinion-ai-energy-use-data-centers-electricity/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Tue, 20 May 2025 09:00:00 +0000",
    "published_datetime": "2025-05-20T10:00:00",
    "summary": "With seemingly no limit to the demand for artificial intelligence, everyone in the energy, AI, and climate fields is justifiably worried. Will there be enough clean electricity to power AI and enough water to cool the data centers that support this technology? These are important questions with serious implications for communities, the economy, and the environment.\u00a0\n\n\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\n\n\nBut the question about AI\u2019s energy usage portends even bigger issues about what we need to do in addressing climate change for the next several decades. If we can\u2019t work out how to handle this, we won\u2019t be able to handle broader electrification of the economy, and the climate risks we face will increase.\nInnovation in IT got us to this point. Graphics processing units (GPUs) that power the computing behind AI have fallen in cost by 99% since 2006. There was similar concern about the energy use of data centers in the early 2010s, with wild projections of growth in electricity demand. But gains in computing power and energy efficiency not only proved these projections wrong but enabled a 550% increase in global computing capability from 2010 to 2018 with only minimal increases in energy use.\u00a0\nIn the late 2010s, however, the trends that had saved us began to break. As the accuracy of AI models dramatically improved, the electricity needed for data centers also started increasing faster; they now account for 4.4% of total demand, up\u00a0 from 1.9% in 2018. Data centers consume more than 10% of the electricity supply in six US states. In Virginia, which has emerged as a hub of data center activity, that figure is 25%.\nProjections about the future demand for energy to power AI are uncertain and range widely, but in one study, Lawrence Berkeley National Laboratory estimated that data centers could represent 6% to 12% of total US electricity use by 2028. Communities and companies will notice this type of rapid growth in electricity demand. It will put pressure on energy prices and on ecosystems. The projections have resulted in calls to build lots of new fossil-fired power plants or bring older ones out of retirement. In many parts of the US, the demand will likely result in a surge of natural-gas-powered plants.\nIt\u2019s a daunting situation. Yet when we zoom out, the projected electricity use from AI is still pretty small. The US generated about 4,300 billion kilowatt-hours last year. We\u2019ll likely need another 1,000 billion to 1,200 billion or more in the next decade\u2014a 24% to 29% increase. Almost half the additional electricity demand will be from electrified vehicles. Another 30% is expected to be from electrified technologies in buildings and industry. Innovation in vehicle and building electrification also advanced in the last decade, and this shift will be good news for the climate, for communities, and for energy costs.\nThe remaining 22% of new electricity demand is estimated to come from AI and data centers. While it represents a smaller piece of the pie, it\u2019s the most urgent one. Because of their rapid growth and geographic concentration, data centers are the electrification challenge we face right now\u2014the small stuff we have to figure out before we\u2019re able to do the big stuff like vehicles and buildings.\nWe also need to understand what the energy consumption and carbon emissions associated with AI are buying us. While the impacts from producing semiconductors and powering AI data centers are important, they are likely small compared with the positive or negative effects AI may have on applications such as the electricity grid, the transportation system, buildings and factories, or consumer behavior. Companies could use AI to develop new materials or batteries that would better integrate renewable energy into the grid. But they could also use AI to make it easier to find more fossil fuels. The claims about potential benefits for the climate are exciting, but they need to be continuously verified and will need support to be realized.\nThis isn\u2019t the first time we\u2019ve faced challenges coping with growth in electricity demand. In the 1960s, US electricity demand was growing at more than 7% per year. In the 1970s that growth was nearly 5%, and in the 1980s and 1990s it was more than 2% per year. Then, starting in 2005, we basically had a decade and a half of flat electricity growth. Most projections for the next decade put our expected growth in electricity demand at around 2% again\u2014but this time we\u2019ll have to do things differently.\u00a0\nTo manage these new energy demands, we need a \u201cGrid New Deal\u201d that leverages public and private capital to rebuild the electricity system for AI with enough capacity and intelligence for decarbonization. New clean energy supplies, investment in transmission and distribution, and strategies for virtual demand management can cut emissions, lower prices, and increase resilience. Data centers bringing clean electricity and distribution system upgrades could be given a fast lane to connect to the grid. Infrastructure banks could fund new transmission lines or pay to upgrade existing ones. Direct investment or tax incentives could encourage clean computing standards, workforce development in the clean energy sector, and open data transparency from data center operators about their energy use so that communities can understand and measure the impacts.\nIn 2022, the White House released a Blueprint for an AI Bill of Rights that provided principles to protect the public\u2019s rights, opportunities, and access to critical resources from being restricted by AI systems. To the AI Bill of Rights, we humbly offer a climate amendment, because ethical AI must be climate-safe AI. It\u2019s a starting point to ensure that the growth of AI works for everyone\u2014that it doesn\u2019t raise people\u2019s energy bills, adds more clean power to the grid than it uses, increases investment in the power system\u2019s infrastructure, and benefits communities while driving innovation.\nBy grounding the conversation about AI and energy in context about what is needed to tackle climate change, we can deliver better outcomes for communities, ecosystems, and the economy. The growth of electricity demand for AI and data centers is a test case for how society will respond to the demands and challenges of broader electrification. If we get this wrong, the likelihood of meeting our climate targets will be extremely low. This is what we mean when we say the energy and climate impacts from data centers are small, but they are also huge.\nCosta Samaras is the Trustee Professor of Civil and Environmental Engineering and director of the Scott Institute for Energy Innovation at Carnegie Mellon University.\nEmma Strubell is the Raj Reddy Assistant Professor in the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University.\nRamayya Krishnan is dean of the Heinz College of Information Systems and Public Policy and the William W. and Ruth F. Cooper Professor of Management Science and Information Systems at Carnegie Mellon University.",
    "content": "With seemingly no limit to the demand for artificial intelligence, everyone in the energy, AI, and climate fields is justifiably worried. Will there be enough clean electricity to power AI and enough water to cool the data centers that support this technology? These are important questions with serious implications for communities, the economy, and the environment.\u00a0\n\n\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\n\n\nBut the question about AI\u2019s energy usage portends even bigger issues about what we need to do in addressing climate change for the next several decades. If we can\u2019t work out how to handle this, we won\u2019t be able to handle broader electrification of the economy, and the climate risks we face will increase.\nInnovation in IT got us to this point. Graphics processing units (GPUs) that power the computing behind AI have fallen in cost by 99% since 2006. There was similar concern about the energy use of data centers in the early 2010s, with wild projections of growth in electricity demand. But gains in computing power and energy efficiency not only proved these projections wrong but enabled a 550% increase in global computing capability from 2010 to 2018 with only minimal increases in energy use.\u00a0\nIn the late 2010s, however, the trends that had saved us began to break. As the accuracy of AI models dramatically improved, the electricity needed for data centers also started increasing faster; they now account for 4.4% of total demand, up\u00a0 from 1.9% in 2018. Data centers consume more than 10% of the electricity supply in six US states. In Virginia, which has emerged as a hub of data center activity, that figure is 25%.\nProjections about the future demand for energy to power AI are uncertain and range widely, but in one study, Lawrence Berkeley National Laboratory estimated that data centers could represent 6% to 12% of total US electricity use by 2028. Communities and companies will notice this type of rapid growth in electricity demand. It will put pressure on energy prices and on ecosystems. The projections have resulted in calls to build lots of new fossil-fired power plants or bring older ones out of retirement. In many parts of the US, the demand will likely result in a surge of natural-gas-powered plants.\nIt\u2019s a daunting situation. Yet when we zoom out, the projected electricity use from AI is still pretty small. The US generated about 4,300 billion kilowatt-hours last year. We\u2019ll likely need another 1,000 billion to 1,200 billion or more in the next decade\u2014a 24% to 29% increase. Almost half the additional electricity demand will be from electrified vehicles. Another 30% is expected to be from electrified technologies in buildings and industry. Innovation in vehicle and building electrification also advanced in the last decade, and this shift will be good news for the climate, for communities, and for energy costs.\nThe remaining 22% of new electricity demand is estimated to come from AI and data centers. While it represents a smaller piece of the pie, it\u2019s the most urgent one. Because of their rapid growth and geographic concentration, data centers are the electrification challenge we face right now\u2014the small stuff we have to figure out before we\u2019re able to do the big stuff like vehicles and buildings.\nWe also need to understand what the energy consumption and carbon emissions associated with AI are buying us. While the impacts from producing semiconductors and powering AI data centers are important, they are likely small compared with the positive or negative effects AI may have on applications such as the electricity grid, the transportation system, buildings and factories, or consumer behavior. Companies could use AI to develop new materials or batteries that would better integrate renewable energy into the grid. But they could also use AI to make it easier to find more fossil fuels. The claims about potential benefits for the climate are exciting, but they need to be continuously verified and will need support to be realized.\nThis isn\u2019t the first time we\u2019ve faced challenges coping with growth in electricity demand. In the 1960s, US electricity demand was growing at more than 7% per year. In the 1970s that growth was nearly 5%, and in the 1980s and 1990s it was more than 2% per year. Then, starting in 2005, we basically had a decade and a half of flat electricity growth. Most projections for the next decade put our expected growth in electricity demand at around 2% again\u2014but this time we\u2019ll have to do things differently.\u00a0\nTo manage these new energy demands, we need a \u201cGrid New Deal\u201d that leverages public and private capital to rebuild the electricity system for AI with enough capacity and intelligence for decarbonization. New clean energy supplies, investment in transmission and distribution, and strategies for virtual demand management can cut emissions, lower prices, and increase resilience. Data centers bringing clean electricity and distribution system upgrades could be given a fast lane to connect to the grid. Infrastructure banks could fund new transmission lines or pay to upgrade existing ones. Direct investment or tax incentives could encourage clean computing standards, workforce development in the clean energy sector, and open data transparency from data center operators about their energy use so that communities can understand and measure the impacts.\nIn 2022, the White House released a Blueprint for an AI Bill of Rights that provided principles to protect the public\u2019s rights, opportunities, and access to critical resources from being restricted by AI systems. To the AI Bill of Rights, we humbly offer a climate amendment, because ethical AI must be climate-safe AI. It\u2019s a starting point to ensure that the growth of AI works for everyone\u2014that it doesn\u2019t raise people\u2019s energy bills, adds more clean power to the grid than it uses, increases investment in the power system\u2019s infrastructure, and benefits communities while driving innovation.\nBy grounding the conversation about AI and energy in context about what is needed to tackle climate change, we can deliver better outcomes for communities, ecosystems, and the economy. The growth of electricity demand for AI and data centers is a test case for how society will respond to the demands and challenges of broader electrification. If we get this wrong, the likelihood of meeting our climate targets will be extremely low. This is what we mean when we say the energy and climate impacts from data centers are small, but they are also huge.\nCosta Samaras is the Trustee Professor of Civil and Environmental Engineering and director of the Scott Institute for Energy Innovation at Carnegie Mellon University.\nEmma Strubell is the Raj Reddy Assistant Professor in the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University.\nRamayya Krishnan is dean of the Heinz College of Information Systems and Public Policy and the William W. and Ruth F. Cooper Professor of Management Science and Information Systems at Carnegie Mellon University."
  },
  {
    "title": "The data center boom in the desert",
    "url": "https://www.technologyreview.com/2025/05/20/1116287/ai-data-centers-nevada-water-reno-computing-environmental-impact/",
    "source": "MIT Technology Review",
    "category": "technology",
    "published": "Tue, 20 May 2025 09:00:00 +0000",
    "published_datetime": "2025-05-20T10:00:00",
    "summary": "In the high desert east of Reno, Nevada, construction crews are flattening the golden foothills of the Virginia Range, laying the foundations of a data center city.\nGoogle, Tract, Switch, EdgeCore, Novva, Vantage, and PowerHouse are all operating, building, or expanding huge facilities within the Tahoe Reno Industrial Center, a business park bigger than the city of Detroit.\u00a0\n\n\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\n\n\nMeanwhile, Microsoft acquired more than 225 acres of undeveloped property within the center and an even larger plot in nearby Silver Springs, Nevada. Apple is expanding its data center, located just across the Truckee River from the industrial park. OpenAI has said it\u2019s considering building a data center in Nevada as well.\nThe corporate race to amass computing resources to train and run artificial intelligence models and store information in the cloud has sparked a data center boom in the desert\u2014just far enough away from Nevada\u2019s communities to elude wide notice and, some fear, adequate scrutiny.\u00a0\n\nSwitch, a data center company based in Las Vegas, says the full build-out of its campus at the Tahoe Reno Industrial Center could exceed seven million square feet.EMILY NAJERA\n\n\nThe full scale and potential environmental impacts of the developments aren\u2019t known, because the footprint, energy needs, and water requirements are often closely guarded corporate secrets. Most of the companies didn\u2019t respond to inquiries from MIT Technology Review, or declined to provide additional information about the projects.\u00a0\nBut there\u2019s \u201ca whole lot of construction going on,\u201d says Kris Thompson, who served as the longtime project manager for the industrial center before stepping down late last year. \u201cThe last number I heard was 13 million square feet under construction right now, which is massive.\u201d\nIndeed, it\u2019s the equivalent of almost five Empire State Buildings laid out flat. In addition, public filings from NV Energy, the state\u2019s near-monopoly utility, reveal that a dozen data-center projects, mostly in this area, have requested nearly six gigawatts of electricity capacity within the next decade.\u00a0\nThat would make the greater Reno area\u2014the biggest little city in the world\u2014one of the largest data-center markets around the globe.\nIt would also require expanding the state\u2019s power sector by about 40%, all for a single industry in an explosive growth stage that may, or may not, prove sustainable. The energy needs, in turn, suggest those projects could consume billions of gallons of water per year, according to an analysis conducted for this story.\u00a0\n\nConstruction crews are busy building data centers throughout the Tahoe Reno Industrial Center.EMILY NAJERA\n\n\nThe build-out of a dense cluster of energy and water-hungry data centers in a small stretch of the nation\u2019s driest state, where climate change is driving up temperatures faster than anywhere else in the country, has begun to raise alarms among water experts, environmental groups, and residents. That includes members of the Pyramid Lake Paiute Tribe, whose namesake water body lies within their reservation and marks the end point of the Truckee River, the region\u2019s main source of water.\nMuch of Nevada has suffered through severe drought conditions for years, farmers and communities are drawing down many of the state\u2019s groundwater reservoirs faster than they can be refilled, and global warming is sucking more and more moisture out of the region\u2019s streams, shrubs, and soils.\n\u201cTelling entities that they can come in and stick more straws in the ground for data centers is raising a lot of questions about sound management,\u201d says Kyle Roerink, executive director of the Great Basin Water Network, a nonprofit that works to protect water resources throughout Nevada and Utah.\u00a0\n\u201cWe just don\u2019t want to be in a situation where the tail is wagging the dog,\u201d he later added, \u201cwhere this demand for data centers is driving water policy.\u201d\nLuring data centers\nIn the late 1850s, the mountains southeast of Reno began enticing prospectors from across the country, who hoped to strike silver or gold in the famed Comstock Lode. But Storey County had few residents or economic prospects by the late 1990s, around the time when Don Roger Norman, a media-shy real estate speculator, spotted a new opportunity in the sagebrush-covered hills.\u00a0\nHe began buying up tens of thousands of acres of land for tens of millions of dollars and lining up development approvals to lure industrial projects to what became the Tahoe Reno Industrial Center. His partners included Lance Gilman, a cowboy-hat-wearing real estate broker, who later bought the nearby Mustang Ranch brothel and won a seat as a county commissioner. \nIn 1999, the county passed an ordinance that preapproves companies to develop most types of commercial and industrial projects across the business park, cutting months to years off the development process. That helped cinch deals with a flock of tenants looking to build big projects fast, including Walmart, Tesla, and Redwood Materials. Now the promise of fast permits is helping to draw data centers by the gigawatt.\nOn a clear, cool January afternoon, Brian Armon, a commercial real estate broker who leads the industrial practices group at NAI Alliance, takes me on a tour of the projects around the region, which mostly entails driving around the business center.\n\nLance Gilman, a local real estate broker, helped to develop the Tahoe Reno Industrial Center and land some of its largest tenants.GREGG SEGAL\n\n\nAfter pulling off Interstate 80 onto USA Parkway, he points out the cranes, earthmovers, and riprap foundations, where a variety of data centers are under construction. Deeper into the industrial park, Armon pulls up near Switch\u2019s long, low, arched-roof facility, which sits on a terrace above cement walls and security gates. The Las Vegas\u2013based company says the first phase of its data center campus encompasses more than a million square feet, and that the full build-out will cover seven times that space.\u00a0\nOver the next hill, we turn around in Google\u2019s parking lot. Cranes, tents, framing, and construction equipment extend behind the company\u2019s existing data center, filling much of the 1,210-acre lot that the search engine giant acquired in 2017.\nLast August, during an event at the University of Nevada, Reno, the company announced it would spend $400 million to expand the data center campus along with another one in Las Vegas.\nThompson says that the development company, Tahoe Reno Industrial LLC, has now sold off every parcel of developable land within the park (although several lots are available for resale following the failed gamble of one crypto tenant).\nWhen I ask Armon what\u2019s attracting all the data centers here, he starts with the fast approvals but cites a list of other lures as well: The inexpensive land. NV Energy\u2019s willingness to strike deals to supply relatively low-cost electricity. Cool nighttime and winter temperatures, as far as American deserts go, which reduce the energy and water needs. The proximity to tech hubs such as Silicon Valley, which cuts latency for applications in which milliseconds matter. And the lack of natural disasters that could shut down the facilities, at least for the most part.\n\u201cWe are high in seismic activity,\u201d he says. \u201cBut everything else is good. We\u2019re not going to have a tornado or flood or a devastating wildfire.\u201dThen there\u2019s the generous tax policies.\nIn 2023, Novva, a Utah-based data center company, announced plans to build a 300,000-square-foot facility within the industrial business park.\nNevada doesn\u2019t charge corporate income tax, and it has also enacted deep tax cuts specifically for data centers that set up shop in the state. That includes abatements of up to 75% on property tax for a decade or two\u2014and nearly as much of a bargain on the sales and use taxes applied to equipment purchased for the facilities.\nData centers don\u2019t require many permanent workers to run the operations, but the projects have created thousands of construction jobs. They\u2019re also helping to diversify the region\u2019s economy beyond casinos and generating tax windfalls for the state, counties, and cities, says Jeff Sutich, executive director of the Northern Nevada Development Authority. Indeed, just three data-center projects, developed by Apple, Google, and Vantage, will produce nearly half a billion dollars in tax revenue for Nevada, even with those generous abatements, according to the Nevada Governor\u2019s Office of Economic Development.\nThe question is whether the benefits of data centers are worth the tradeoffs for Nevadans, given the public health costs, greenhouse-gas emissions, energy demands, and water strains.\nThe rain shadow\nThe Sierra Nevada\u2019s granite peaks trace the eastern edge of California, forcing Pacific Ocean winds to rise and cool. That converts water vapor in the air into the rain and snow that fill the range\u2019s tributaries, rivers, and lakes.\u00a0\nBut the same meteorological phenomenon casts a rain shadow over much of neighboring Nevada, forming an arid expanse known as the Great Basin Desert. The state receives about 10 inches of precipitation a year, about a third of the national average.\nThe Truckee River draws from the melting Sierra snowpack at the edge of Lake Tahoe, cascades down the range, and snakes through the flatlands of Reno and Sparks. It forks at the Derby Dam, a Reclamation Act project a few miles from the Tahoe Reno Industrial Center, which diverts water to a farming region further east while allowing the rest to continue north toward Pyramid Lake.\u00a0\nAlong the way, an engineered system of reservoirs, canals, and treatment plants divert, store, and release water from the river, supplying businesses, cities, towns, and native tribes across the region. But Nevada\u2019s population and economy are expanding, creating more demands on these resources even as they become more constrained.\u00a0\n\nThe Truckee River, which originates at Lake Tahoe and terminates at Pyramid Lake, is the major water source for cities, towns, and farms across northwestern Nevada.EMILY NAJERA\n\n\nThroughout much of the 2020s the state has suffered through one of the hottest and most widespread droughts on record, extending two decades of abnormally dry conditions across the American West. Some scientists fear it may constitute an emerging megadrought.\u00a0\nAbout 50% of Nevada currently faces moderate to exceptional drought conditions. In addition, more than half of the state\u2019s hundreds of groundwater basins are already \u201cover-appropriated,\u201d meaning the water rights on paper exceed the levels believed to be underground.\u00a0\nIt\u2019s not clear if climate change will increase or decrease the state\u2019s rainfall levels, on balance. But precipitation patterns are expected to become more erratic, whiplashing between short periods of intense rainfall and more-frequent, extended, or severe droughts.\u00a0\nIn addition, more precipitation will fall as rain rather than snow, shortening the Sierra snow season by weeks to months over the coming decades.\u00a0\n\u201cIn the extreme case, at the end of the century, that\u2019s pretty much all of winter,\u201d says Sean McKenna, executive director of hydrologic sciences at the Desert Research Institute, a research division of the Nevada System of Higher Education.\nThat loss will undermine an essential function of the Sierra snowpack: reliably delivering water to farmers and cities when it\u2019s most needed in the spring and summer, across both Nevada and California.\u00a0\nThese shifting conditions will require the region to develop better ways to store, preserve, and recycle the water it does get, McKenna says. Northern Nevada\u2019s cities, towns, and agencies will also need to carefully evaluate and plan for the collective impacts of continuing growth and development on the interconnected water system, particularly when it comes to water-hungry projects like data centers, he adds.\u201cWe can\u2019t consider each of these as a one-off, without considering that there may be tens or dozens of these in the next 15 years,\u201d McKenna says.\nThirsty data centers\nData centers suck up water in two main ways.\nAs giant rooms of server racks process information and consume energy, they generate heat that must be shunted away to prevent malfunctions and damage to the equipment. The processing units optimized for training and running AI models often draw more electricity and, in turn, produce more heat.\nTo keep things cool, more and more data centers have turned to liquid cooling systems that don\u2019t need as much electricity as fan cooling or air-conditioning.\nThese often rely on water to absorb heat and transfer it to outdoor cooling towers, where much of the moisture evaporates. Microsoft\u2019s US data centers, for instance, could have directly evaporated nearly 185,000 gallons of \u201cclean freshwater\u201d in the course of training OpenAI\u2019s GPT-3 large language model, according to a 2023 preprint study led by researchers at the University of California, Riverside. (The research has since been peer-reviewed and is awaiting publication.)\nWhat\u2019s less appreciated, however, is that the larger data-center drain on water generally occurs indirectly, at the power plants generating extra electricity for the turbocharged AI sector. These facilities, in turn, require more water to cool down equipment, among other purposes. \n\nYou have to add up both uses \u201cto reflect the true water cost of data centers,\u201d says Shaolei Ren, an associate professor of electrical and computer engineering at UC Riverside and coauthor of the study.\nRen estimates that the 12 data-center projects listed in NV Energy\u2019s report would directly consume between 860 million gallons and 5.7 billion gallons a year, based on the requested electricity capacity. (\u201cConsumed\u201d here means the water is evaporated, not merely withdrawn and returned to the engineered water system.) The indirect water drain associated with electricity generation for those operations could add up to 15.5 billion gallons, based on the average consumption of the regional grid.\nThe exact water figures would depend on shifting climate conditions, the type of cooling systems each data center uses, and the mix of power sources that supply the facilities.\nSolar power, which provides roughly a quarter of Nevada\u2019s power, requires relatively little water to operate, for instance. But natural-gas plants, which generate about 56%, withdraw 2,803 gallons per megawatt-hour on average, according to the Energy Information Administration.\u00a0\nGeothermal plants, which produce about 10% of the state\u2019s electricity by cycling water through hot rocks, generally consume less water than fossil fuel plants do but often require more water than other renewables, according to some research.\u00a0\nBut here too, the water usage varies depending on the type of geothermal plant in question. Google has lined up several deals to partially power its data centers through Fervo Energy, which has helped to commercialize an emerging approach that injects water under high pressure to fracture rock and form wells deep below the surface.\u00a0\nThe company stresses that it doesn\u2019t evaporate water for cooling and that it relies on brackish groundwater, not fresh water, to develop and run its plants. In a recent post, Fervo noted that its facilities consume significantly less water per megawatt-hour than coal, nuclear, or natural-gas plants do.\nPart of NV Energy\u2019s proposed plan to meet growing electricity demands in Nevada includes developing several natural-gas peaking units, adding more than one gigawatt of solar power and installing another gigawatt of battery storage. It\u2019s also forging ahead with a more than $4 billion transmission project.But the company didn\u2019t respond to questions concerning how it will supply all of the gigawatts of additional electricity requested by data centers, if the construction of those power plants will increase consumer rates, or how much water those facilities are expected to consume.\n\nNV Energy operates a transmission line, substation, and power plant in or around the Tahoe Reno Industrial Center.EMILY NAJERA\n\n\n\u201cNV Energy teams work diligently on our long-term planning to make investments in our infrastructure to serve new customers and the continued growth in the state without putting existing customers at risk,\u201d the company said in a statement.\nAn added challenge is that data centers need to run around the clock. That will often compel utilities to develop new electricity-generating sources that can run nonstop as well, as natural-gas, geothermal, or nuclear plants do, says Emily Grubert, an associate professor of sustainable energy policy at the University of Notre Dame, who has studied the relative water consumption of electricity sources.\u00a0\n\u201cYou end up with the water-intensive resources looking more important,\u201d she adds.\nEven if NV Energy and the companies developing data centers do strive to power them through sources with relatively low water needs, \u201cwe only have so much ability to add six gigawatts to Nevada\u2019s grid,\u201d Grubert explains. \u201cWhat you do will never be system-neutral, because it\u2019s such a big number.\u201d\nSecuring supplies\nOn a mid-February morning, I meet TRI\u2019s Thompson and Don Gilman, Lance Gilman\u2019s son, at the Storey County offices, located within the industrial center.\u00a0\n\u201cI\u2019m just a country boy who sells dirt,\u201d Gilman, also a real estate broker, says by way of introduction.\u00a0\nWe climb into his large SUV and drive to a reservoir in the heart of the industrial park, filled nearly to the lip.\u00a0\nThompson explains that much of the water comes from an on-site treatment facility that filters waste fluids from companies in the park. In addition, tens of millions of gallons of treated effluent will also likely flow into the tank this year from the Truckee Meadows Water Authority Reclamation Facility, near the border of Reno and Sparks. That\u2019s thanks to a 16-mile pipeline that the developers, the water authority, several tenants, and various local cities and agencies partnered to build, through a project that began in 2021.\n\u201cOur general improvement district is furnishing that water to tech companies here in the park as we speak,\u201d Thompson says. \u201cThat helps preserve the precious groundwater, so that is an environmental feather in the cap for these data centers. They are focused on environmental excellence.\u201d\n\nThe reservoir within the industrial business park provides water to data centers and other tenants.EMILY NAJERA\n\n\nBut data centers often need drinking-quality water\u2014not wastewater merely treated to irrigation standards\u2014for evaporative cooling, \u201cto avoid pipe clogs and/or bacterial growth,\u201d the UC Riverside study notes. For instance, Google says its data centers withdrew about 7.7 billion gallons of water in 2023, and nearly 6 billion of those gallons were potable.\u00a0\nTenants in the industrial park can potentially obtain access to water from the ground and the Truckee River, as well. From early on, the master developers worked hard to secure permits to water sources, since they are nearly as precious as development entitlements to companies hoping to build projects in the desert.\nInitially, the development company controlled a private business, the TRI Water and Sewer Company, that provided those services to the business park\u2019s tenants, according to public documents. The company set up wells, a water tank, distribution lines, and a sewer disposal system.\u00a0\nBut in 2000, the board of county commissioners established a general improvement district, a legal mechanism for providing municipal services in certain parts of the state, to manage electricity and then water within the center. It, in turn, hired TRI Water and Sewer as the operating company.\nAs of its 2020 service plan, the general improvement district held permits for nearly 5,300 acre-feet of groundwater, \u201cwhich can be pumped from well fields within the service area and used for new growth as it occurs.\u201d The document lists another 2,000 acre-feet per year available from the on-site treatment facility, 1,000 from the Truckee River, and 4,000 more from the effluent pipeline.\u00a0\nThose figures haven\u2019t budged much since, according to Shari Whalen, general manager of the TRI General Improvement District. All told, they add up to more than 4 billion gallons of water per year for all the needs of the industrial park and the tenants there, data centers and otherwise.\nWhalen says that the amount and quality of water required for any given data center depends on its design, and that those matters are worked out on a case-by-case basis.\u00a0\nWhen asked if the general improvement district is confident that it has adequate water resources to supply the needs of all the data centers under development, as well as other tenants at the industrial center, she says: \u201cThey can\u2019t just show up and build unless they have water resources designated for their projects. We wouldn\u2019t approve a project if it didn\u2019t have those water resources.\u201d\nWater battles\nAs the region\u2019s water sources have grown more constrained, lining up supplies has become an increasingly high-stakes and controversial business.\nMore than a century ago, the US federal government filed a lawsuit against an assortment of parties pulling water from the Truckee River. The suit would eventually establish that the Pyramid Lake Paiute Tribe\u2019s legal rights to water for irrigation superseded other claims. But the tribe has been fighting to protect those rights and increase flows from the river ever since, arguing that increasing strains on the watershed from upstream cities and businesses threaten to draw away water reserved for reservation farming, decrease lake levels, and harm native fish.\nThe Pyramid Lake Paiute Tribe considers the water body and its fish, including the endangered cui-ui and threatened Lahontan cutthroat trout, to be essential parts of its culture, identity, and way of life. The tribe was originally named Cui-ui Ticutta, which translates to cui-ui eaters. The lake continues to provide sustenance as well as business for the tribe and its members, a number of whom operate boat charters and fishing guide services.\n\u201cIt\u2019s completely tied into us as a people,\u201d says Steven Wadsworth, chairman of the Pyramid Lake Paiute Tribe.\n\u201cThat is what has sustained us all this time,\u201d he adds. \u201cIt\u2019s just who we are. It\u2019s part of our spiritual well-being.\u201d\n\nSteven Wadsworth, chairman of the Pyramid Lake Paiute Tribe, fears that data centers will divert water that would otherwise reach the tribe\u2019s namesake lake.EMILY NAJERA\n\n\nIn recent decades, the tribe has sued the Nevada State Engineer, Washoe County, the federal government, and others for overallocating water rights and endangering the lake\u2019s fish. It also protested the TRI General Improvement District\u2019s applications to draw thousands of additional acre\u2011feet of groundwater from a basin near the business park. In 2019, the State Engineer\u2019s office rejected those requests, concluding that the basin was already fully appropriated.\u00a0\nMore recently, the tribe took issue with the plan to build the pipeline and divert effluent that would have flown into the Truckee, securing an agreement that required the Truckee Meadows Water Authority and other parties to add back several thousand acre\u2011feet of water to the river.\u00a0\nWhalen says she\u2019s sensitive to Wadsworth\u2019s concerns. But she says that the pipeline promises to keep a growing amount of treated wastewater out of the river, where it could otherwise contribute to rising salt levels in the lake.\n\u201cI think that the pipeline from [the Truckee Meadows Water Authority] to our system is good for water quality in the river,\u201d she says. \u201cI understand philosophically the concerns about data centers, but the general improvement district is dedicated to working with everyone on the river for regional water-resource planning\u2014and the tribe is no exception.\u201d\nWater efficiency\u00a0\nIn an email, Thompson added that he has \u201cgreat respect and admiration,\u201d for the tribe and has visited the reservation several times in an effort to help bring industrial or commercial development there.He stressed that all of the business park\u2019s groundwater was \u201cvalidated by the State Water Engineer,\u201d and that the rights to surface water and effluent were purchased \u201cfor fair market value.\u201d\nDuring the earlier interview at the industrial center, he and Gilman had both expressed confidence that tenants in the park have adequate water supplies, and that the businesses won\u2019t draw water away from other areas.\u00a0\n\u201cWe\u2019re in our own aquifer, our own water basin here,\u201d Thompson said. \u201cYou put a straw in the ground here, you\u2019re not going to pull water from Fernley or from Reno or from Silver Springs.\u201d\nGilman also stressed that data-center companies have gotten more water efficient in recent years, echoing a point others made as well.\n\u201cWith the newer technology, it\u2019s not much of a worry,\u201d says Sutich, of the Northern Nevada Development Authority. \u201cThe technology has come a long way in the last 10 years, which is really giving these guys the opportunity to be good stewards of water usage.\u201d\n\nAn aerial view of the cooling tower fans at Google\u2019s data center in the Tahoe Reno Industrial Center.GOOGLE\n\n\nIndeed, Google\u2019s existing Storey County facility is air-cooled, according to the company\u2019s latest environmental report. The data center withdrew 1.9 million gallons in 2023 but only consumed 200,000 gallons. The rest cycles back into the water system.\nGoogle said all the data centers under construction on its campus will also \u201cutilize air-cooling technology.\u201d The company didn\u2019t respond to a question about the scale of its planned expansion in the Tahoe Reno Industrial Center, and referred a question about indirect water consumption to NV Energy.\nThe search giant has stressed that it strives to be water efficient across all of its data centers, and decides whether to use air or liquid cooling based on local supply and projected demand, among other variables.Four years ago, the company set a goal of replenishing more water than it consumes by 2030. Locally, it also committed to provide half a million dollars to the National Forest Foundation to improve the Truckee River watershed and reduce wildfire risks.\u00a0\nMicrosoft clearly suggested in earlier news reports that the Silver Springs land it purchased around the end of 2022 would be used for a data center. NAI Alliance\u2019s market real estate report identifies that lot, as well as the parcel Microsoft purchased within the Tahoe Reno Industrial Center, as data center sites.\nBut the company now declines to specify what it intends to build in the region.\u00a0\n\u201cWhile the land purchase is public knowledge, we have not disclosed specific details [of] our plans for the land or potential development timelines,\u201d wrote Donna Whitehead, a Microsoft spokesperson, in an email.\u00a0\n\nWorkers have begun grading land inside a fenced off lot within the Tahoe Reno Industrial Center.EMILY NAJERA\n\n\nMicrosoft has also scaled down its global data-center ambitions, backing away from several projects in recent months amid shifting economic conditions, according to various reports.\nWhatever it ultimately does or doesn\u2019t build, the company stresses that it has made strides to reduce water consumption in its facilities. Late last year, the company announced that it\u2019s using \u201cchip-level cooling solutions\u201d in data centers, which continually circulate water between the servers and chillers through a closed loop that the company claims doesn\u2019t lose any water to evaporation. It says the design requires only a \u201cnominal increase\u201d in energy compared to its data centers that rely on evaporative water cooling.\nOthers seem to be taking a similar approach. EdgeCore also said its 900,000-square-foot data center at the Tahoe Reno Industrial Center will rely on an \u201cair-cooled closed-loop chiller\u201d that doesn\u2019t require water evaporation for cooling.\u00a0\nBut some of the companies seem to have taken steps to ensure access to significant amounts of water. Switch, for instance, took a lead role in developing the effluent pipeline. In addition, Tract, which develops campuses on which third-party data centers can build their own facilities, has said it lined up more than 1,100 acre-feet of water rights, the equivalent of nearly 360 million gallons a year.\u00a0\nApple, Novva, Switch, Tract, and Vantage didn\u2019t respond to inquiries from MIT Technology Review.\u00a0\nComing conflicts\u00a0\nThe suggestion that companies aren\u2019t straining water supplies when they adopt air cooling is, in many cases, akin to saying they\u2019re not responsible for the greenhouse gas produced through their power use simply because it occurs outside of their facilities. In fact, the additional water used at a power plant to meet the increased electricity needs of air cooling may exceed any gains at the data center, Ren, of UC Riverside, says.\n\u201cThat\u2019s actually very likely, because it uses a lot more energy,\u201d he adds.\nThat means that some of the companies developing data centers in and around Storey County may simply hand off their water challenges to other parts of Nevada or neighboring states across the drying American West, depending on where and how the power is generated, Ren says.\u00a0\nGoogle has said its air-cooled facilities require about 10% more electricity, and its environmental report notes that the Storey County facility is one of its two least-energy-efficient data centers.\u00a0\n\nPipes running along Google\u2019s data center campus help the search company cool its servers.GOOGLE\n\n\nSome fear there\u2019s also a growing mismatch between what Nevada\u2019s water permits allow, what\u2019s actually in the ground, and what nature will provide as climate conditions shift. Notably, the groundwater committed to all parties from the Tracy Segment basin\u2014a long-fought-over resource that partially supplies the TRI General Improvement District\u2014already exceeds the \u201cperennial yield.\u201d That refers to the maximum amount that can be drawn out every year without depleting the reservoir over the long term.\u201cIf pumping does ultimately exceed the available supply, that means there will be conflict among users,\u201d Roerink, of the Great Basin Water Network, said in an email. \u201cSo I have to wonder: Who could be suing whom? Who could be buying out whom? How will the tribe\u2019s rights be defended?\u201d\nThe Truckee Meadows Water Authority, the community-owned utility that manages the water system for Reno and Sparks, said it is planning carefully for the future and remains confident there will be \u201csufficient resources for decades to come,\u201d at least within its territory east of the industrial center.\nStorey County, the Truckee-Carson Irrigation District, and the State Engineer\u2019s office didn\u2019t respond to questions or accept interview requests.\u00a0\nOpen for business\nAs data center proposals have begun shifting into Northern Nevada\u2019s cities, more local residents and organizations have begun to take notice and express concerns. The regional division of the Sierra Club, for instance, recently sought to overturn the approval of Reno\u2019s first data center, about 20 miles west of the Tahoe Reno Industrial Center.\u00a0\nOlivia Tanager, director of the Sierra Club\u2019s Toiyabe Chapter, says the environmental organization was shocked by the projected electricity demands from data centers highlighted in NV Energy\u2019s filings.\n\nNevada\u2019s wild horses are a common sight along USA Parkway, the highway cutting through the industrial business park.\u00a0EMILY NAJERA\n\n\n\u201cWe have increasing interest in understanding the impact that data centers will have to our climate goals, to our grid as a whole, and certainly to our water resources,\u201d she says. \u201cThe demands are extraordinary, and we don\u2019t have that amount of water to toy around with.\u201d\nDuring a city hall hearing in January that stretched late into the evening, she and a line of residents raised concerns about the water, energy, climate, and employment impacts of AI data centers. At the end, though, the city council upheld the planning department\u2019s approval of the project, on a 5-2 vote.\n\u201cWelcome to Reno,\u201d Kathleen Taylor, Reno\u2019s vice mayor, said before casting her vote. \u201cWe\u2019re open for business.\u201d\nWhere the river ends\nIn late March, I walk alongside Chairman Wadsworth, of the Pyramid Lake Paiute Tribe, on the shores of Pyramid Lake, watching a row of fly-fishers in waders cast their lines into the cold waters.\u00a0\nThe lake is the largest remnant of Lake Lahontan, an Ice Age inland sea that once stretched across western Nevada and would have submerged present-day Reno. But as the climate warmed, the lapping waters retreated, etching erosional terraces into the mountainsides and exposing tufa deposits around the lake, large formations of porous rock made of calcium-carbonate. That includes the pyramid-shaped island on the eastern shore that inspired the lake\u2019s name.\nA lone angler stands along the shores of Pyramid Lake.\nIn the decades after the US Reclamation Service completed the Derby Dam in 1905, Pyramid Lake declined another 80 feet and nearby Winnemucca Lake dried up entirely.\u201cWe know what happens when water use goes unchecked,\u201d says Wadsworth, gesturing eastward toward the range across the lake, where Winnemucca once filled the next basin over. \u201cBecause all we have to do is look over there and see a dry, barren lake bed that used to be full.\u201dIn an earlier interview, Wadsworth acknowledged that the world needs data centers. But he argued they should be spread out across the country, not densely clustered in the middle of the Nevada desert.\nGiven the fierce competition for resources up to now, he can\u2019t imagine how there could be enough water to meet the demands of data centers, expanding cities, and other growing businesses without straining the limited local supplies that should, by his accounting, flow to Pyramid Lake.\nHe fears these growing pressures will force the tribe to wage new legal battles to protect their rights and preserve the lake, extending what he refers to as \u201ca century of water wars.\u201d\n\u201cWe have seen the devastating effects of what happens when you mess with Mother Nature,\u201d Wadsworth says. \u201cPart of our spirit has left us. And that\u2019s why we fight so hard to hold on to what\u2019s left.\u201d",
    "content": "In the high desert east of Reno, Nevada, construction crews are flattening the golden foothills of the Virginia Range, laying the foundations of a data center city.\nGoogle, Tract, Switch, EdgeCore, Novva, Vantage, and PowerHouse are all operating, building, or expanding huge facilities within the Tahoe Reno Industrial Center, a business park bigger than the city of Detroit.\u00a0\n\n\nThis story is a part of MIT Technology Review\u2019s series \u201cPower Hungry: AI and our energy future,\u201d on the energy demands and carbon costs of the artificial-intelligence revolution.\n\n\nMeanwhile, Microsoft acquired more than 225 acres of undeveloped property within the center and an even larger plot in nearby Silver Springs, Nevada. Apple is expanding its data center, located just across the Truckee River from the industrial park. OpenAI has said it\u2019s considering building a data center in Nevada as well.\nThe corporate race to amass computing resources to train and run artificial intelligence models and store information in the cloud has sparked a data center boom in the desert\u2014just far enough away from Nevada\u2019s communities to elude wide notice and, some fear, adequate scrutiny.\u00a0\n\nSwitch, a data center company based in Las Vegas, says the full build-out of its campus at the Tahoe Reno Industrial Center could exceed seven million square feet.EMILY NAJERA\n\n\nThe full scale and potential environmental impacts of the developments aren\u2019t known, because the footprint, energy needs, and water requirements are often closely guarded corporate secrets. Most of the companies didn\u2019t respond to inquiries from MIT Technology Review, or declined to provide additional information about the projects.\u00a0\nBut there\u2019s \u201ca whole lot of construction going on,\u201d says Kris Thompson, who served as the longtime project manager for the industrial center before stepping down late last year. \u201cThe last number I heard was 13 million square feet under construction right now, which is massive.\u201d\nIndeed, it\u2019s the equivalent of almost five Empire State Buildings laid out flat. In addition, public filings from NV Energy, the state\u2019s near-monopoly utility, reveal that a dozen data-center projects, mostly in this area, have requested nearly six gigawatts of electricity capacity within the next decade.\u00a0\nThat would make the greater Reno area\u2014the biggest little city in the world\u2014one of the largest data-center markets around the globe.\nIt would also require expanding the state\u2019s power sector by about 40%, all for a single industry in an explosive growth stage that may, or may not, prove sustainable. The energy needs, in turn, suggest those projects could consume billions of gallons of water per year, according to an analysis conducted for this story.\u00a0\n\nConstruction crews are busy building data centers throughout the Tahoe Reno Industrial Center.EMILY NAJERA\n\n\nThe build-out of a dense cluster of energy and water-hungry data centers in a small stretch of the nation\u2019s driest state, where climate change is driving up temperatures faster than anywhere else in the country, has begun to raise alarms among water experts, environmental groups, and residents. That includes members of the Pyramid Lake Paiute Tribe, whose namesake water body lies within their reservation and marks the end point of the Truckee River, the region\u2019s main source of water.\nMuch of Nevada has suffered through severe drought conditions for years, farmers and communities are drawing down many of the state\u2019s groundwater reservoirs faster than they can be refilled, and global warming is sucking more and more moisture out of the region\u2019s streams, shrubs, and soils.\n\u201cTelling entities that they can come in and stick more straws in the ground for data centers is raising a lot of questions about sound management,\u201d says Kyle Roerink, executive director of the Great Basin Water Network, a nonprofit that works to protect water resources throughout Nevada and Utah.\u00a0\n\u201cWe just don\u2019t want to be in a situation where the tail is wagging the dog,\u201d he later added, \u201cwhere this demand for data centers is driving water policy.\u201d\nLuring data centers\nIn the late 1850s, the mountains southeast of Reno began enticing prospectors from across the country, who hoped to strike silver or gold in the famed Comstock Lode. But Storey County had few residents or economic prospects by the late 1990s, around the time when Don Roger Norman, a media-shy real estate speculator, spotted a new opportunity in the sagebrush-covered hills.\u00a0\nHe began buying up tens of thousands of acres of land for tens of millions of dollars and lining up development approvals to lure industrial projects to what became the Tahoe Reno Industrial Center. His partners included Lance Gilman, a cowboy-hat-wearing real estate broker, who later bought the nearby Mustang Ranch brothel and won a seat as a county commissioner. \nIn 1999, the county passed an ordinance that preapproves companies to develop most types of commercial and industrial projects across the business park, cutting months to years off the development process. That helped cinch deals with a flock of tenants looking to build big projects fast, including Walmart, Tesla, and Redwood Materials. Now the promise of fast permits is helping to draw data centers by the gigawatt.\nOn a clear, cool January afternoon, Brian Armon, a commercial real estate broker who leads the industrial practices group at NAI Alliance, takes me on a tour of the projects around the region, which mostly entails driving around the business center.\n\nLance Gilman, a local real estate broker, helped to develop the Tahoe Reno Industrial Center and land some of its largest tenants.GREGG SEGAL\n\n\nAfter pulling off Interstate 80 onto USA Parkway, he points out the cranes, earthmovers, and riprap foundations, where a variety of data centers are under construction. Deeper into the industrial park, Armon pulls up near Switch\u2019s long, low, arched-roof facility, which sits on a terrace above cement walls and security gates. The Las Vegas\u2013based company says the first phase of its data center campus encompasses more than a million square feet, and that the full build-out will cover seven times that space.\u00a0\nOver the next hill, we turn around in Google\u2019s parking lot. Cranes, tents, framing, and construction equipment extend behind the company\u2019s existing data center, filling much of the 1,210-acre lot that the search engine giant acquired in 2017.\nLast August, during an event at the University of Nevada, Reno, the company announced it would spend $400 million to expand the data center campus along with another one in Las Vegas.\nThompson says that the development company, Tahoe Reno Industrial LLC, has now sold off every parcel of developable land within the park (although several lots are available for resale following the failed gamble of one crypto tenant).\nWhen I ask Armon what\u2019s attracting all the data centers here, he starts with the fast approvals but cites a list of other lures as well: The inexpensive land. NV Energy\u2019s willingness to strike deals to supply relatively low-cost electricity. Cool nighttime and winter temperatures, as far as American deserts go, which reduce the energy and water needs. The proximity to tech hubs such as Silicon Valley, which cuts latency for applications in which milliseconds matter. And the lack of natural disasters that could shut down the facilities, at least for the most part.\n\u201cWe are high in seismic activity,\u201d he says. \u201cBut everything else is good. We\u2019re not going to have a tornado or flood or a devastating wildfire.\u201dThen there\u2019s the generous tax policies.\nIn 2023, Novva, a Utah-based data center company, announced plans to build a 300,000-square-foot facility within the industrial business park.\nNevada doesn\u2019t charge corporate income tax, and it has also enacted deep tax cuts specifically for data centers that set up shop in the state. That includes abatements of up to 75% on property tax for a decade or two\u2014and nearly as much of a bargain on the sales and use taxes applied to equipment purchased for the facilities.\nData centers don\u2019t require many permanent workers to run the operations, but the projects have created thousands of construction jobs. They\u2019re also helping to diversify the region\u2019s economy beyond casinos and generating tax windfalls for the state, counties, and cities, says Jeff Sutich, executive director of the Northern Nevada Development Authority. Indeed, just three data-center projects, developed by Apple, Google, and Vantage, will produce nearly half a billion dollars in tax revenue for Nevada, even with those generous abatements, according to the Nevada Governor\u2019s Office of Economic Development.\nThe question is whether the benefits of data centers are worth the tradeoffs for Nevadans, given the public health costs, greenhouse-gas emissions, energy demands, and water strains.\nThe rain shadow\nThe Sierra Nevada\u2019s granite peaks trace the eastern edge of California, forcing Pacific Ocean winds to rise and cool. That converts water vapor in the air into the rain and snow that fill the range\u2019s tributaries, rivers, and lakes.\u00a0\nBut the same meteorological phenomenon casts a rain shadow over much of neighboring Nevada, forming an arid expanse known as the Great Basin Desert. The state receives about 10 inches of precipitation a year, about a third of the national average.\nThe Truckee River draws from the melting Sierra snowpack at the edge of Lake Tahoe, cascades down the range, and snakes through the flatlands of Reno and Sparks. It forks at the Derby Dam, a Reclamation Act project a few miles from the Tahoe Reno Industrial Center, which diverts water to a farming region further east while allowing the rest to continue north toward Pyramid Lake.\u00a0\nAlong the way, an engineered system of reservoirs, canals, and treatment plants divert, store, and release water from the river, supplying businesses, cities, towns, and native tribes across the region. But Nevada\u2019s population and economy are expanding, creating more demands on these resources even as they become more constrained.\u00a0\n\nThe Truckee River, which originates at Lake Tahoe and terminates at Pyramid Lake, is the major water source for cities, towns, and farms across northwestern Nevada.EMILY NAJERA\n\n\nThroughout much of the 2020s the state has suffered through one of the hottest and most widespread droughts on record, extending two decades of abnormally dry conditions across the American West. Some scientists fear it may constitute an emerging megadrought.\u00a0\nAbout 50% of Nevada currently faces moderate to exceptional drought conditions. In addition, more than half of the state\u2019s hundreds of groundwater basins are already \u201cover-appropriated,\u201d meaning the water rights on paper exceed the levels believed to be underground.\u00a0\nIt\u2019s not clear if climate change will increase or decrease the state\u2019s rainfall levels, on balance. But precipitation patterns are expected to become more erratic, whiplashing between short periods of intense rainfall and more-frequent, extended, or severe droughts.\u00a0\nIn addition, more precipitation will fall as rain rather than snow, shortening the Sierra snow season by weeks to months over the coming decades.\u00a0\n\u201cIn the extreme case, at the end of the century, that\u2019s pretty much all of winter,\u201d says Sean McKenna, executive director of hydrologic sciences at the Desert Research Institute, a research division of the Nevada System of Higher Education.\nThat loss will undermine an essential function of the Sierra snowpack: reliably delivering water to farmers and cities when it\u2019s most needed in the spring and summer, across both Nevada and California.\u00a0\nThese shifting conditions will require the region to develop better ways to store, preserve, and recycle the water it does get, McKenna says. Northern Nevada\u2019s cities, towns, and agencies will also need to carefully evaluate and plan for the collective impacts of continuing growth and development on the interconnected water system, particularly when it comes to water-hungry projects like data centers, he adds.\u201cWe can\u2019t consider each of these as a one-off, without considering that there may be tens or dozens of these in the next 15 years,\u201d McKenna says.\nThirsty data centers\nData centers suck up water in two main ways.\nAs giant rooms of server racks process information and consume energy, they generate heat that must be shunted away to prevent malfunctions and damage to the equipment. The processing units optimized for training and running AI models often draw more electricity and, in turn, produce more heat.\nTo keep things cool, more and more data centers have turned to liquid cooling systems that don\u2019t need as much electricity as fan cooling or air-conditioning.\nThese often rely on water to absorb heat and transfer it to outdoor cooling towers, where much of the moisture evaporates. Microsoft\u2019s US data centers, for instance, could have directly evaporated nearly 185,000 gallons of \u201cclean freshwater\u201d in the course of training OpenAI\u2019s GPT-3 large language model, according to a 2023 preprint study led by researchers at the University of California, Riverside. (The research has since been peer-reviewed and is awaiting publication.)\nWhat\u2019s less appreciated, however, is that the larger data-center drain on water generally occurs indirectly, at the power plants generating extra electricity for the turbocharged AI sector. These facilities, in turn, require more water to cool down equipment, among other purposes. \n\nYou have to add up both uses \u201cto reflect the true water cost of data centers,\u201d says Shaolei Ren, an associate professor of electrical and computer engineering at UC Riverside and coauthor of the study.\nRen estimates that the 12 data-center projects listed in NV Energy\u2019s report would directly consume between 860 million gallons and 5.7 billion gallons a year, based on the requested electricity capacity. (\u201cConsumed\u201d here means the water is evaporated, not merely withdrawn and returned to the engineered water system.) The indirect water drain associated with electricity generation for those operations could add up to 15.5 billion gallons, based on the average consumption of the regional grid.\nThe exact water figures would depend on shifting climate conditions, the type of cooling systems each data center uses, and the mix of power sources that supply the facilities.\nSolar power, which provides roughly a quarter of Nevada\u2019s power, requires relatively little water to operate, for instance. But natural-gas plants, which generate about 56%, withdraw 2,803 gallons per megawatt-hour on average, according to the Energy Information Administration.\u00a0\nGeothermal plants, which produce about 10% of the state\u2019s electricity by cycling water through hot rocks, generally consume less water than fossil fuel plants do but often require more water than other renewables, according to some research.\u00a0\nBut here too, the water usage varies depending on the type of geothermal plant in question. Google has lined up several deals to partially power its data centers through Fervo Energy, which has helped to commercialize an emerging approach that injects water under high pressure to fracture rock and form wells deep below the surface.\u00a0\nThe company stresses that it doesn\u2019t evaporate water for cooling and that it relies on brackish groundwater, not fresh water, to develop and run its plants. In a recent post, Fervo noted that its facilities consume significantly less water per megawatt-hour than coal, nuclear, or natural-gas plants do.\nPart of NV Energy\u2019s proposed plan to meet growing electricity demands in Nevada includes developing several natural-gas peaking units, adding more than one gigawatt of solar power and installing another gigawatt of battery storage. It\u2019s also forging ahead with a more than $4 billion transmission project.But the company didn\u2019t respond to questions concerning how it will supply all of the gigawatts of additional electricity requested by data centers, if the construction of those power plants will increase consumer rates, or how much water those facilities are expected to consume.\n\nNV Energy operates a transmission line, substation, and power plant in or around the Tahoe Reno Industrial Center.EMILY NAJERA\n\n\n\u201cNV Energy teams work diligently on our long-term planning to make investments in our infrastructure to serve new customers and the continued growth in the state without putting existing customers at risk,\u201d the company said in a statement.\nAn added challenge is that data centers need to run around the clock. That will often compel utilities to develop new electricity-generating sources that can run nonstop as well, as natural-gas, geothermal, or nuclear plants do, says Emily Grubert, an associate professor of sustainable energy policy at the University of Notre Dame, who has studied the relative water consumption of electricity sources.\u00a0\n\u201cYou end up with the water-intensive resources looking more important,\u201d she adds.\nEven if NV Energy and the companies developing data centers do strive to power them through sources with relatively low water needs, \u201cwe only have so much ability to add six gigawatts to Nevada\u2019s grid,\u201d Grubert explains. \u201cWhat you do will never be system-neutral, because it\u2019s such a big number.\u201d\nSecuring supplies\nOn a mid-February morning, I meet TRI\u2019s Thompson and Don Gilman, Lance Gilman\u2019s son, at the Storey County offices, located within the industrial center.\u00a0\n\u201cI\u2019m just a country boy who sells dirt,\u201d Gilman, also a real estate broker, says by way of introduction.\u00a0\nWe climb into his large SUV and drive to a reservoir in the heart of the industrial park, filled nearly to the lip.\u00a0\nThompson explains that much of the water comes from an on-site treatment facility that filters waste fluids from companies in the park. In addition, tens of millions of gallons of treated effluent will also likely flow into the tank this year from the Truckee Meadows Water Authority Reclamation Facility, near the border of Reno and Sparks. That\u2019s thanks to a 16-mile pipeline that the developers, the water authority, several tenants, and various local cities and agencies partnered to build, through a project that began in 2021.\n\u201cOur general improvement district is furnishing that water to tech companies here in the park as we speak,\u201d Thompson says. \u201cThat helps preserve the precious groundwater, so that is an environmental feather in the cap for these data centers. They are focused on environmental excellence.\u201d\n\nThe reservoir within the industrial business park provides water to data centers and other tenants.EMILY NAJERA\n\n\nBut data centers often need drinking-quality water\u2014not wastewater merely treated to irrigation standards\u2014for evaporative cooling, \u201cto avoid pipe clogs and/or bacterial growth,\u201d the UC Riverside study notes. For instance, Google says its data centers withdrew about 7.7 billion gallons of water in 2023, and nearly 6 billion of those gallons were potable.\u00a0\nTenants in the industrial park can potentially obtain access to water from the ground and the Truckee River, as well. From early on, the master developers worked hard to secure permits to water sources, since they are nearly as precious as development entitlements to companies hoping to build projects in the desert.\nInitially, the development company controlled a private business, the TRI Water and Sewer Company, that provided those services to the business park\u2019s tenants, according to public documents. The company set up wells, a water tank, distribution lines, and a sewer disposal system.\u00a0\nBut in 2000, the board of county commissioners established a general improvement district, a legal mechanism for providing municipal services in certain parts of the state, to manage electricity and then water within the center. It, in turn, hired TRI Water and Sewer as the operating company.\nAs of its 2020 service plan, the general improvement district held permits for nearly 5,300 acre-feet of groundwater, \u201cwhich can be pumped from well fields within the service area and used for new growth as it occurs.\u201d The document lists another 2,000 acre-feet per year available from the on-site treatment facility, 1,000 from the Truckee River, and 4,000 more from the effluent pipeline.\u00a0\nThose figures haven\u2019t budged much since, according to Shari Whalen, general manager of the TRI General Improvement District. All told, they add up to more than 4 billion gallons of water per year for all the needs of the industrial park and the tenants there, data centers and otherwise.\nWhalen says that the amount and quality of water required for any given data center depends on its design, and that those matters are worked out on a case-by-case basis.\u00a0\nWhen asked if the general improvement district is confident that it has adequate water resources to supply the needs of all the data centers under development, as well as other tenants at the industrial center, she says: \u201cThey can\u2019t just show up and build unless they have water resources designated for their projects. We wouldn\u2019t approve a project if it didn\u2019t have those water resources.\u201d\nWater battles\nAs the region\u2019s water sources have grown more constrained, lining up supplies has become an increasingly high-stakes and controversial business.\nMore than a century ago, the US federal government filed a lawsuit against an assortment of parties pulling water from the Truckee River. The suit would eventually establish that the Pyramid Lake Paiute Tribe\u2019s legal rights to water for irrigation superseded other claims. But the tribe has been fighting to protect those rights and increase flows from the river ever since, arguing that increasing strains on the watershed from upstream cities and businesses threaten to draw away water reserved for reservation farming, decrease lake levels, and harm native fish.\nThe Pyramid Lake Paiute Tribe considers the water body and its fish, including the endangered cui-ui and threatened Lahontan cutthroat trout, to be essential parts of its culture, identity, and way of life. The tribe was originally named Cui-ui Ticutta, which translates to cui-ui eaters. The lake continues to provide sustenance as well as business for the tribe and its members, a number of whom operate boat charters and fishing guide services.\n\u201cIt\u2019s completely tied into us as a people,\u201d says Steven Wadsworth, chairman of the Pyramid Lake Paiute Tribe.\n\u201cThat is what has sustained us all this time,\u201d he adds. \u201cIt\u2019s just who we are. It\u2019s part of our spiritual well-being.\u201d\n\nSteven Wadsworth, chairman of the Pyramid Lake Paiute Tribe, fears that data centers will divert water that would otherwise reach the tribe\u2019s namesake lake.EMILY NAJERA\n\n\nIn recent decades, the tribe has sued the Nevada State Engineer, Washoe County, the federal government, and others for overallocating water rights and endangering the lake\u2019s fish. It also protested the TRI General Improvement District\u2019s applications to draw thousands of additional acre\u2011feet of groundwater from a basin near the business park. In 2019, the State Engineer\u2019s office rejected those requests, concluding that the basin was already fully appropriated.\u00a0\nMore recently, the tribe took issue with the plan to build the pipeline and divert effluent that would have flown into the Truckee, securing an agreement that required the Truckee Meadows Water Authority and other parties to add back several thousand acre\u2011feet of water to the river.\u00a0\nWhalen says she\u2019s sensitive to Wadsworth\u2019s concerns. But she says that the pipeline promises to keep a growing amount of treated wastewater out of the river, where it could otherwise contribute to rising salt levels in the lake.\n\u201cI think that the pipeline from [the Truckee Meadows Water Authority] to our system is good for water quality in the river,\u201d she says. \u201cI understand philosophically the concerns about data centers, but the general improvement district is dedicated to working with everyone on the river for regional water-resource planning\u2014and the tribe is no exception.\u201d\nWater efficiency\u00a0\nIn an email, Thompson added that he has \u201cgreat respect and admiration,\u201d for the tribe and has visited the reservation several times in an effort to help bring industrial or commercial development there.He stressed that all of the business park\u2019s groundwater was \u201cvalidated by the State Water Engineer,\u201d and that the rights to surface water and effluent were purchased \u201cfor fair market value.\u201d\nDuring the earlier interview at the industrial center, he and Gilman had both expressed confidence that tenants in the park have adequate water supplies, and that the businesses won\u2019t draw water away from other areas.\u00a0\n\u201cWe\u2019re in our own aquifer, our own water basin here,\u201d Thompson said. \u201cYou put a straw in the ground here, you\u2019re not going to pull water from Fernley or from Reno or from Silver Springs.\u201d\nGilman also stressed that data-center companies have gotten more water efficient in recent years, echoing a point others made as well.\n\u201cWith the newer technology, it\u2019s not much of a worry,\u201d says Sutich, of the Northern Nevada Development Authority. \u201cThe technology has come a long way in the last 10 years, which is really giving these guys the opportunity to be good stewards of water usage.\u201d\n\nAn aerial view of the cooling tower fans at Google\u2019s data center in the Tahoe Reno Industrial Center.GOOGLE\n\n\nIndeed, Google\u2019s existing Storey County facility is air-cooled, according to the company\u2019s latest environmental report. The data center withdrew 1.9 million gallons in 2023 but only consumed 200,000 gallons. The rest cycles back into the water system.\nGoogle said all the data centers under construction on its campus will also \u201cutilize air-cooling technology.\u201d The company didn\u2019t respond to a question about the scale of its planned expansion in the Tahoe Reno Industrial Center, and referred a question about indirect water consumption to NV Energy.\nThe search giant has stressed that it strives to be water efficient across all of its data centers, and decides whether to use air or liquid cooling based on local supply and projected demand, among other variables.Four years ago, the company set a goal of replenishing more water than it consumes by 2030. Locally, it also committed to provide half a million dollars to the National Forest Foundation to improve the Truckee River watershed and reduce wildfire risks.\u00a0\nMicrosoft clearly suggested in earlier news reports that the Silver Springs land it purchased around the end of 2022 would be used for a data center. NAI Alliance\u2019s market real estate report identifies that lot, as well as the parcel Microsoft purchased within the Tahoe Reno Industrial Center, as data center sites.\nBut the company now declines to specify what it intends to build in the region.\u00a0\n\u201cWhile the land purchase is public knowledge, we have not disclosed specific details [of] our plans for the land or potential development timelines,\u201d wrote Donna Whitehead, a Microsoft spokesperson, in an email.\u00a0\n\nWorkers have begun grading land inside a fenced off lot within the Tahoe Reno Industrial Center.EMILY NAJERA\n\n\nMicrosoft has also scaled down its global data-center ambitions, backing away from several projects in recent months amid shifting economic conditions, according to various reports.\nWhatever it ultimately does or doesn\u2019t build, the company stresses that it has made strides to reduce water consumption in its facilities. Late last year, the company announced that it\u2019s using \u201cchip-level cooling solutions\u201d in data centers, which continually circulate water between the servers and chillers through a closed loop that the company claims doesn\u2019t lose any water to evaporation. It says the design requires only a \u201cnominal increase\u201d in energy compared to its data centers that rely on evaporative water cooling.\nOthers seem to be taking a similar approach. EdgeCore also said its 900,000-square-foot data center at the Tahoe Reno Industrial Center will rely on an \u201cair-cooled closed-loop chiller\u201d that doesn\u2019t require water evaporation for cooling.\u00a0\nBut some of the companies seem to have taken steps to ensure access to significant amounts of water. Switch, for instance, took a lead role in developing the effluent pipeline. In addition, Tract, which develops campuses on which third-party data centers can build their own facilities, has said it lined up more than 1,100 acre-feet of water rights, the equivalent of nearly 360 million gallons a year.\u00a0\nApple, Novva, Switch, Tract, and Vantage didn\u2019t respond to inquiries from MIT Technology Review.\u00a0\nComing conflicts\u00a0\nThe suggestion that companies aren\u2019t straining water supplies when they adopt air cooling is, in many cases, akin to saying they\u2019re not responsible for the greenhouse gas produced through their power use simply because it occurs outside of their facilities. In fact, the additional water used at a power plant to meet the increased electricity needs of air cooling may exceed any gains at the data center, Ren, of UC Riverside, says.\n\u201cThat\u2019s actually very likely, because it uses a lot more energy,\u201d he adds.\nThat means that some of the companies developing data centers in and around Storey County may simply hand off their water challenges to other parts of Nevada or neighboring states across the drying American West, depending on where and how the power is generated, Ren says.\u00a0\nGoogle has said its air-cooled facilities require about 10% more electricity, and its environmental report notes that the Storey County facility is one of its two least-energy-efficient data centers.\u00a0\n\nPipes running along Google\u2019s data center campus help the search company cool its servers.GOOGLE\n\n\nSome fear there\u2019s also a growing mismatch between what Nevada\u2019s water permits allow, what\u2019s actually in the ground, and what nature will provide as climate conditions shift. Notably, the groundwater committed to all parties from the Tracy Segment basin\u2014a long-fought-over resource that partially supplies the TRI General Improvement District\u2014already exceeds the \u201cperennial yield.\u201d That refers to the maximum amount that can be drawn out every year without depleting the reservoir over the long term.\u201cIf pumping does ultimately exceed the available supply, that means there will be conflict among users,\u201d Roerink, of the Great Basin Water Network, said in an email. \u201cSo I have to wonder: Who could be suing whom? Who could be buying out whom? How will the tribe\u2019s rights be defended?\u201d\nThe Truckee Meadows Water Authority, the community-owned utility that manages the water system for Reno and Sparks, said it is planning carefully for the future and remains confident there will be \u201csufficient resources for decades to come,\u201d at least within its territory east of the industrial center.\nStorey County, the Truckee-Carson Irrigation District, and the State Engineer\u2019s office didn\u2019t respond to questions or accept interview requests.\u00a0\nOpen for business\nAs data center proposals have begun shifting into Northern Nevada\u2019s cities, more local residents and organizations have begun to take notice and express concerns. The regional division of the Sierra Club, for instance, recently sought to overturn the approval of Reno\u2019s first data center, about 20 miles west of the Tahoe Reno Industrial Center.\u00a0\nOlivia Tanager, director of the Sierra Club\u2019s Toiyabe Chapter, says the environmental organization was shocked by the projected electricity demands from data centers highlighted in NV Energy\u2019s filings.\n\nNevada\u2019s wild horses are a common sight along USA Parkway, the highway cutting through the industrial business park.\u00a0EMILY NAJERA\n\n\n\u201cWe have increasing interest in understanding the impact that data centers will have to our climate goals, to our grid as a whole, and certainly to our water resources,\u201d she says. \u201cThe demands are extraordinary, and we don\u2019t have that amount of water to toy around with.\u201d\nDuring a city hall hearing in January that stretched late into the evening, she and a line of residents raised concerns about the water, energy, climate, and employment impacts of AI data centers. At the end, though, the city council upheld the planning department\u2019s approval of the project, on a 5-2 vote.\n\u201cWelcome to Reno,\u201d Kathleen Taylor, Reno\u2019s vice mayor, said before casting her vote. \u201cWe\u2019re open for business.\u201d\nWhere the river ends\nIn late March, I walk alongside Chairman Wadsworth, of the Pyramid Lake Paiute Tribe, on the shores of Pyramid Lake, watching a row of fly-fishers in waders cast their lines into the cold waters.\u00a0\nThe lake is the largest remnant of Lake Lahontan, an Ice Age inland sea that once stretched across western Nevada and would have submerged present-day Reno. But as the climate warmed, the lapping waters retreated, etching erosional terraces into the mountainsides and exposing tufa deposits around the lake, large formations of porous rock made of calcium-carbonate. That includes the pyramid-shaped island on the eastern shore that inspired the lake\u2019s name.\nA lone angler stands along the shores of Pyramid Lake.\nIn the decades after the US Reclamation Service completed the Derby Dam in 1905, Pyramid Lake declined another 80 feet and nearby Winnemucca Lake dried up entirely.\u201cWe know what happens when water use goes unchecked,\u201d says Wadsworth, gesturing eastward toward the range across the lake, where Winnemucca once filled the next basin over. \u201cBecause all we have to do is look over there and see a dry, barren lake bed that used to be full.\u201dIn an earlier interview, Wadsworth acknowledged that the world needs data centers. But he argued they should be spread out across the country, not densely clustered in the middle of the Nevada desert.\nGiven the fierce competition for resources up to now, he can\u2019t imagine how there could be enough water to meet the demands of data centers, expanding cities, and other growing businesses without straining the limited local supplies that should, by his accounting, flow to Pyramid Lake.\nHe fears these growing pressures will force the tribe to wage new legal battles to protect their rights and preserve the lake, extending what he refers to as \u201ca century of water wars.\u201d\n\u201cWe have seen the devastating effects of what happens when you mess with Mother Nature,\u201d Wadsworth says. \u201cPart of our spirit has left us. And that\u2019s why we fight so hard to hold on to what\u2019s left.\u201d"
  }
]